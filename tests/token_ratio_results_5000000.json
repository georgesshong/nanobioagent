{
  "test_date": "2025-09-02T11:51:50.174719",
  "total_models": 9,
  "successful_models": 0,
  "models_with_ratios": 7,
  "results": [
    {
      "model_name": "gpt-4.1-mini",
      "success": false,
      "prompt_length_chars": 5000014,
      "tokens_used": 1250032,
      "token_limit": 1000000,
      "char_to_token_ratio": 3.99990880233466,
      "error_message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4.1-mini-long-context in organization org-cRHHMQpiXlqYHk5TCJ1EUUT0 on tokens per min (TPM): Limit 1000000, Requested 1250032. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "response": null,
      "model_config": {
        "cut_length": 72000,
        "max_tokens": 512,
        "model_type": "openai",
        "context_window": 1047576,
        "temperature": 0,
        "doc": "https://platform.openai.com/docs/models/gpt-4.1-mini",
        "pricing": {
          "input": 0.4,
          "output": 1.6,
          "per_tokens": 1000000
        },
        "_metadata": {
          "matched_key": "gpt-4.1-mini",
          "auto_detected_nvidia": false,
          "calculated_cut_length": 72000
        }
      }
    },
    {
      "model_name": "gpt-4.1-nano",
      "success": false,
      "prompt_length_chars": 5000014,
      "tokens_used": 1250032,
      "token_limit": 1000000,
      "char_to_token_ratio": 3.99990880233466,
      "error_message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4.1-nano-long-context in organization org-cRHHMQpiXlqYHk5TCJ1EUUT0 on tokens per min (TPM): Limit 1000000, Requested 1250032. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "response": null,
      "model_config": {
        "cut_length": 72000,
        "max_tokens": 512,
        "model_type": "openai",
        "context_window": 1047576,
        "temperature": 0,
        "doc": "https://platform.openai.com/docs/models/gpt-4.1-nano",
        "pricing": {
          "input": 0.1,
          "output": 0.4,
          "per_tokens": 1000000
        },
        "_metadata": {
          "matched_key": "gpt-4.1-nano",
          "auto_detected_nvidia": false,
          "calculated_cut_length": 72000
        }
      }
    },
    {
      "model_name": "gpt-4.1",
      "success": false,
      "prompt_length_chars": 5000014,
      "tokens_used": 1250032,
      "token_limit": 500000,
      "char_to_token_ratio": 3.99990880233466,
      "error_message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4.1-long-context in organization org-cRHHMQpiXlqYHk5TCJ1EUUT0 on tokens per min (TPM): Limit 500000, Requested 1250032. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "response": null,
      "model_config": {
        "cut_length": 36000,
        "max_tokens": 512,
        "model_type": "openai",
        "context_window": 1047576,
        "temperature": 0,
        "doc": "https://platform.openai.com/docs/models/gpt-4.1",
        "pricing": {
          "input": 2,
          "output": 8,
          "per_tokens": 1000000
        },
        "_metadata": {
          "matched_key": "gpt-4.1",
          "auto_detected_nvidia": false,
          "calculated_cut_length": 36000
        }
      }
    },
    {
      "model_name": "gemini-1.5-flash",
      "success": false,
      "prompt_length_chars": 5000014,
      "tokens_used": 2114730,
      "token_limit": 1048575,
      "char_to_token_ratio": 2.364374648300256,
      "error_message": "Invalid argument provided to Gemini: 400 The input token count (2114730) exceeds the maximum number of tokens allowed (1048575).",
      "response": null,
      "model_config": {
        "cut_length": 30000,
        "max_tokens": 512,
        "model_type": "google",
        "context_window": 1000000,
        "temperature": 0,
        "doc": "https://ai.google.dev/gemini-api/docs/pricing#gemini-1.5-flash",
        "pricing": {
          "input": 0.075,
          "output": 0.3,
          "per_tokens": 1000000
        },
        "_metadata": {
          "matched_key": "gemini-1.5-flash",
          "auto_detected_nvidia": false,
          "calculated_cut_length": 30000
        }
      }
    },
    {
      "model_name": "gemini-1.5-flash-8b",
      "success": false,
      "prompt_length_chars": 5000014,
      "tokens_used": 2114730,
      "token_limit": 1048575,
      "char_to_token_ratio": 2.364374648300256,
      "error_message": "Invalid argument provided to Gemini: 400 The input token count (2114730) exceeds the maximum number of tokens allowed (1048575).",
      "response": null,
      "model_config": {
        "cut_length": 30000,
        "max_tokens": 512,
        "model_type": "google",
        "context_window": 1000000,
        "temperature": 0,
        "doc": "https://ai.google.dev/gemini-api/docs/pricing#gemini-1.5-flash-8b",
        "pricing": {
          "input": 0.0375,
          "output": 0.15,
          "per_tokens": 1000000
        },
        "_metadata": {
          "matched_key": "gemini-1.5-flash-8b",
          "auto_detected_nvidia": false,
          "calculated_cut_length": 30000
        }
      }
    },
    {
      "model_name": "gemini-2.0-flash",
      "success": false,
      "prompt_length_chars": 5000014,
      "tokens_used": 2279164,
      "token_limit": 1048575,
      "char_to_token_ratio": 2.193792987253221,
      "error_message": "Invalid argument provided to Gemini: 400 The input token count (2279164) exceeds the maximum number of tokens allowed (1048575).",
      "response": null,
      "model_config": {
        "cut_length": 30000,
        "max_tokens": 512,
        "model_type": "google",
        "context_window": 1048576,
        "temperature": 0,
        "doc": "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.0-flash",
        "pricing": {
          "input": 0.1,
          "output": 0.4,
          "per_tokens": 1000000
        },
        "_metadata": {
          "matched_key": "gemini-2.0-flash",
          "auto_detected_nvidia": false,
          "calculated_cut_length": 30000
        }
      }
    },
    {
      "model_name": "gemini-2.0-flash-lite",
      "success": false,
      "prompt_length_chars": 5000014,
      "tokens_used": 2279164,
      "token_limit": 1048575,
      "char_to_token_ratio": 2.193792987253221,
      "error_message": "Invalid argument provided to Gemini: 400 The input token count (2279164) exceeds the maximum number of tokens allowed (1048575).",
      "response": null,
      "model_config": {
        "cut_length": 30000,
        "max_tokens": 512,
        "model_type": "google",
        "context_window": 1048576,
        "temperature": 0,
        "doc": "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.0-flash-lite",
        "pricing": {
          "input": 0.075,
          "output": 0.3,
          "per_tokens": 1000000
        },
        "_metadata": {
          "matched_key": "gemini-2.0-flash-lite",
          "auto_detected_nvidia": false,
          "calculated_cut_length": 30000
        }
      }
    },
    {
      "model_name": "gemini-2.5-flash",
      "success": false,
      "prompt_length_chars": 5000014,
      "tokens_used": null,
      "token_limit": null,
      "char_to_token_ratio": null,
      "error_message": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 1000000\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 13\n}\n]",
      "response": null,
      "model_config": {
        "cut_length": 96000,
        "max_tokens": 512,
        "model_type": "google",
        "context_window": 1048576,
        "temperature": 0,
        "doc": "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-flash",
        "pricing": {
          "input": 0.3,
          "output": 2.5,
          "per_tokens": 1000000
        },
        "_metadata": {
          "matched_key": "gemini-2.5-flash",
          "auto_detected_nvidia": false,
          "calculated_cut_length": 96000
        }
      }
    },
    {
      "model_name": "gemini-2.5-flash-lite",
      "success": false,
      "prompt_length_chars": 5000014,
      "tokens_used": null,
      "token_limit": null,
      "char_to_token_ratio": null,
      "error_message": "Invalid argument provided to Gemini: 400 The input token count exceeds the maximum number of tokens allowed (1048576).",
      "response": null,
      "model_config": {
        "cut_length": 96000,
        "max_tokens": 512,
        "model_type": "google",
        "context_window": 1048576,
        "temperature": 0,
        "doc": "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-flash-lite",
        "pricing": {
          "input": 0.1,
          "output": 0.3,
          "per_tokens": 1000000
        },
        "_metadata": {
          "matched_key": "gemini-2.5-flash-lite",
          "auto_detected_nvidia": false,
          "calculated_cut_length": 96000
        }
      }
    }
  ]
}