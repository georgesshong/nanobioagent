{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09b5875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssg_h\\anaconda3\\envs\\genegpt_env_39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())  # project root\n",
    "sys.path.insert(0, parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc70229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_supported(models, api_key=None, max_tokens=512):\n",
    "    \"\"\"\n",
    "    Test if HuggingFace model(s) are supported using the same method as call_langchain_model.\n",
    "    \n",
    "    Args:\n",
    "        models (str or list): Single model name or list of model names to test\n",
    "        api_key (str, optional): HuggingFace API key. If None, uses HUGGINGFACE_API_KEY env var\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary of results with same format regardless of input type\n",
    "    \"\"\"\n",
    "    # Convert single string to list\n",
    "    if isinstance(models, str):\n",
    "        model_list = [models]\n",
    "    else:\n",
    "        model_list = models\n",
    "    \n",
    "    # Get API key\n",
    "    if api_key is None:\n",
    "        api_key = os.environ.get('HUGGINGFACE_API_KEY')\n",
    "    \n",
    "    if not api_key:\n",
    "        return {\n",
    "            \"total_tested\": len(model_list),\n",
    "            \"supported_count\": 0,\n",
    "            \"correct_count\": 0,\n",
    "            \"supported_models\": [],\n",
    "            \"correct_models\": [],\n",
    "            \"detailed_results\": {model: {\n",
    "                \"supported\": False,\n",
    "                \"response\": \"\",\n",
    "                \"correct_answer\": False,\n",
    "                \"error\": \"No API key provided\"\n",
    "            } for model in model_list}\n",
    "        }\n",
    "    \n",
    "    # Import LangChain components (same as your actual code)\n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        from langchain_core.messages import SystemMessage, HumanMessage\n",
    "    except ImportError:\n",
    "        return {\n",
    "            \"total_tested\": len(model_list),\n",
    "            \"supported_count\": 0,\n",
    "            \"correct_count\": 0,\n",
    "            \"supported_models\": [],\n",
    "            \"correct_models\": [],\n",
    "            \"detailed_results\": {model: {\n",
    "                \"supported\": False,\n",
    "                \"response\": \"\",\n",
    "                \"correct_answer\": False,\n",
    "                \"error\": \"LangChain imports not available\"\n",
    "            } for model in model_list}\n",
    "        }\n",
    "    \n",
    "    results = {}\n",
    "    supported_models = []\n",
    "    correct_models = []\n",
    "    \n",
    "    # Define stop sequences (same as your actual code)\n",
    "    STOP_SEQUENCE = ['->', '\\n\\nQuestion']\n",
    "    \n",
    "    print(f\"Testing {len(model_list)} HuggingFace model(s)...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for model_name in model_list:\n",
    "        if model_name.startswith(\"huggingface/\"):\n",
    "            model_name = model_name.replace(\"huggingface/\", \"\")\n",
    "        print(f\"Testing: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Create ChatOpenAI model (same as your actual implementation)\n",
    "            llm = ChatOpenAI(\n",
    "                model=model_name,\n",
    "                temperature=0.0,\n",
    "                openai_api_key=api_key,\n",
    "                openai_api_base=\"https://router.huggingface.co/v1\",\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            # Create messages (same format as call_langchain_model)\n",
    "            system_message = \"You are a calculator. Only output numbers, never show your thinking process.\"\n",
    "            q_prompt = \"You are a calculator. Only output numbers, never show your thinking process. What is 1+1? (just give me the answer)\"\n",
    "            \n",
    "            messages = [\n",
    "                SystemMessage(content=system_message),\n",
    "                HumanMessage(content=q_prompt)\n",
    "            ]\n",
    "            \n",
    "            # Call model (same as call_langchain_model)\n",
    "            response = llm.invoke(messages, stop=STOP_SEQUENCE)\n",
    "            response_text = response.content\n",
    "            \n",
    "            # Handle None response\n",
    "            if response_text is None:\n",
    "                response_text = \"\"\n",
    "            else:\n",
    "                response_text = response_text.strip()\n",
    "            \n",
    "            # For models that use thinking tags, try to extract the final answer\n",
    "            final_answer = response_text\n",
    "            if \"<think>\" in response_text:\n",
    "                # Try to get text after </think> tag\n",
    "                if \"</think>\" in response_text:\n",
    "                    final_answer = response_text.split(\"</think>\")[-1].strip()\n",
    "                else:\n",
    "                    # If thinking tag is not closed, this model might not be suitable\n",
    "                    # But let's still check the original response\n",
    "                    pass\n",
    "            \n",
    "            # Check if the answer is correct using the final answer\n",
    "            response_lower = final_answer.lower()\n",
    "            \n",
    "            # Multiple ways to check for correct answer\n",
    "            correct_answer = (\n",
    "                \"2\" in final_answer or                     # Direct \"2\"\n",
    "                \"two\" in response_lower or                 # Written as \"two\" \n",
    "                final_answer.strip() == \"2\" or             # Exact match\n",
    "                final_answer.strip().endswith(\"2\") or      # Ends with 2\n",
    "                \"1+1=2\" in final_answer.replace(\" \", \"\") or  # Shows equation\n",
    "                \"answer is 2\" in response_lower or         # \"The answer is 2\"\n",
    "                \"result is 2\" in response_lower            # \"The result is 2\"\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                \"supported\": True,\n",
    "                \"response\": response_text,\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"error\": None\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Parse common error types\n",
    "            if \"not supported\" in error_msg.lower():\n",
    "                error_type = \"Model not supported\"\n",
    "            elif \"402\" in error_msg or \"credits\" in error_msg.lower():\n",
    "                error_type = \"Credits exhausted\"\n",
    "            elif \"401\" in error_msg or \"unauthorized\" in error_msg.lower():\n",
    "                error_type = \"Invalid API key\"\n",
    "            elif \"429\" in error_msg or \"rate limit\" in error_msg.lower():\n",
    "                error_type = \"Rate limited\"\n",
    "            else:\n",
    "                error_type = \"Unknown error\"\n",
    "            \n",
    "            result = {\n",
    "                \"supported\": False,\n",
    "                \"response\": \"\",\n",
    "                \"correct_answer\": False,\n",
    "                \"error\": f\"{error_type}: {error_msg}\"\n",
    "            }\n",
    "        \n",
    "        # Store result\n",
    "        results[model_name] = result\n",
    "        \n",
    "        # Update counters and lists\n",
    "        if result[\"supported\"]:\n",
    "            supported_models.append(model_name)\n",
    "            status = f\"✅ SUPPORTED - Response: '{result['response']}'\"\n",
    "            if result[\"correct_answer\"]:\n",
    "                correct_models.append(model_name)\n",
    "                status += \" (CORRECT ✓)\"\n",
    "            else:\n",
    "                status += \" (WRONG ✗)\"\n",
    "        else:\n",
    "            status = f\"❌ FAILED - {result['error']}\"\n",
    "        \n",
    "        print(f\"   {status}\")\n",
    "        print()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Summary:\")\n",
    "    print(f\"  Total tested: {len(model_list)}\")\n",
    "    print(f\"  Supported: {len(supported_models)}\")\n",
    "    print(f\"  Correct answers: {len(correct_models)}\")\n",
    "    \n",
    "    if correct_models:\n",
    "        print(f\"\\n✅ Working models:\")\n",
    "        for model in correct_models:\n",
    "            print(f\"  - {model}\")\n",
    "    \n",
    "    return {\n",
    "        \"total_tested\": len(model_list),\n",
    "        \"supported_count\": len(supported_models),\n",
    "        \"correct_count\": len(correct_models),\n",
    "        \"supported_models\": supported_models,\n",
    "        \"correct_models\": correct_models,\n",
    "        \"detailed_results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d1c64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze_author_models\n",
    "def extract_parameters(model_name):\n",
    "    \"\"\"Extract number of parameters from model name and convert to integer\"\"\"\n",
    "    # Look for patterns like 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B, 110B\n",
    "    pattern = r'(\\d+(?:\\.\\d+)?)[Bb]'\n",
    "    match = re.search(pattern, model_name)\n",
    "    \n",
    "    if match:\n",
    "        size_str = match.group(1)\n",
    "        size_float = float(size_str)\n",
    "        # Convert to actual number (B = billion)\n",
    "        return int(size_float * 1_000_000_000)\n",
    "    \n",
    "    return None  # Unknown size\n",
    "\n",
    "def is_instruct_model(model_info):\n",
    "    instruct_keywords = {\"instruct\", \"chat\", \"conversation\", \"conversational\", \"instruction-tuned\"}\n",
    "    # check tags first\n",
    "    if model_info.tags and any(tag.lower() in instruct_keywords for tag in model_info.tags):\n",
    "        return True\n",
    "    # fallback: check ID if tags are missing\n",
    "    return any(keyword in model_info.modelId.lower() for keyword in instruct_keywords)\n",
    "\n",
    "\n",
    "def analyze_author_models(author_name, top_n=100, min_model_downloads=1):\n",
    "    \"\"\"Analyze models for a specific author\"\"\"\n",
    "    api = HfApi()\n",
    "    \n",
    "    print(f\"Fetching {author_name} text-generation models...\")\n",
    "    \n",
    "    try:\n",
    "        models = api.list_models(\n",
    "            author=author_name, \n",
    "            # task=\"text-generation\",\n",
    "            limit=None,\n",
    "            sort=\"lastModified\",\n",
    "            direction=-1\n",
    "        )\n",
    "        \n",
    "        # Filter for likely chat/instruct models\n",
    "        good_models = []\n",
    "        \n",
    "        for model in models:\n",
    "            # Look for instruct/chat indicators\n",
    "            #is_instruct = any(keyword in model.id.lower() for keyword in [\n",
    "            #    'instruct', 'chat', 'conversation', 'conversational', 'text-generation-inference'\n",
    "            #])\n",
    "            is_instruct = is_instruct_model(model)\n",
    "            print(f\"Checking {model.id}: is_instruct={is_instruct}, downloads={model.downloads}, private={model.private}\")\n",
    "            # Check if it has good stats and is public\n",
    "            if is_instruct and not model.private and model.downloads > min_model_downloads:\n",
    "                good_models.append({\n",
    "                    'id': model.id,\n",
    "                    'created_at': model.created_at,\n",
    "                    'downloads': model.downloads,\n",
    "                    'likes': model.likes,\n",
    "                    'tags': model.tags\n",
    "                })\n",
    "        \n",
    "        # Sort by creation date (newest first)\n",
    "        good_models.sort(key=lambda x: x['created_at'] if x['created_at'] else datetime.min, reverse=True)\n",
    "        \n",
    "        # Take top N models\n",
    "        top_models = good_models[:top_n]\n",
    "        \n",
    "        if not top_models:\n",
    "            print(f\"No suitable models found for {author_name}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Found {len(top_models)} {author_name} models to analyze\")\n",
    "        \n",
    "        # Test all models for support\n",
    "        model_ids_for_testing = [model['id'] for model in top_models]\n",
    "        print(f\"Testing {len(model_ids_for_testing)} models for HuggingFace support...\")\n",
    "        \n",
    "        # Test the models\n",
    "        support_result = check_model_supported(model_ids_for_testing)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df_data = []\n",
    "        \n",
    "        for model in top_models:\n",
    "            model_name = model['id']\n",
    "            \n",
    "            # Extract just the model name (remove author prefix)\n",
    "            display_name = model_name.split('/')[-1]\n",
    "            \n",
    "            # Create URL\n",
    "            url = f\"https://huggingface.co/{model_name}\"\n",
    "            \n",
    "            # Extract parameters\n",
    "            parameters = extract_parameters(model_name)\n",
    "            \n",
    "            # Format release date\n",
    "            released = model['created_at'].strftime('%Y-%m-%d') if model['created_at'] else None\n",
    "            \n",
    "            # Downloads\n",
    "            downloads = model['downloads']\n",
    "            \n",
    "            # Check if supported\n",
    "            supported = model_name in support_result['correct_models']\n",
    "            \n",
    "            df_data.append({\n",
    "                'author': author_name,\n",
    "                'model_name': display_name,\n",
    "                'url': url,\n",
    "                'parameters': parameters,\n",
    "                'released': released,\n",
    "                'downloads': downloads,\n",
    "                'supported': supported\n",
    "            })\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(df_data)\n",
    "        \n",
    "        # Sort by release date (newest first) and then by downloads\n",
    "        df['released_dt'] = pd.to_datetime(df['released'])\n",
    "        df = df.sort_values(['released_dt', 'downloads'], ascending=[False, False])\n",
    "        \n",
    "        # Drop the helper column\n",
    "        df = df.drop('released_dt', axis=1)\n",
    "        \n",
    "        print(f\"{author_name} Analysis Summary:\")\n",
    "        print(f\"  Total models: {len(df)}\")\n",
    "        print(f\"  Supported models: {df['supported'].sum()}\")\n",
    "        print(f\"  Models with known parameters: {df['parameters'].notna().sum()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {author_name}: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d32ddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'primary_task': 'image-text-to-text', 'all_tags': ['transformers', 'safetensors', 'gemma3', 'image-text-to-text', 'conversational', 'arxiv:1905.07830', 'arxiv:1905.10044', 'arxiv:1911.11641', 'arxiv:1904.09728', 'arxiv:1705.03551', 'arxiv:1911.01547', 'arxiv:1907.10641', 'arxiv:1903.00161', 'arxiv:2009.03300', 'arxiv:2304.06364', 'arxiv:2103.03874', 'arxiv:2110.14168', 'arxiv:2311.12022', 'arxiv:2108.07732', 'arxiv:2107.03374', 'arxiv:2210.03057', 'arxiv:2106.03193', 'arxiv:1910.11856', 'arxiv:2502.12404', 'arxiv:2502.21228', 'arxiv:2404.16816', 'arxiv:2104.12756', 'arxiv:2311.16502', 'arxiv:2203.10244', 'arxiv:2404.12390', 'arxiv:1810.12440', 'arxiv:1908.02660', 'arxiv:2312.11805', 'base_model:google/gemma-3-27b-pt', 'base_model:finetune:google/gemma-3-27b-pt', 'license:gemma', 'text-generation-inference', 'endpoints_compatible', 'region:us']}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "def get_model_tasks(model_name: str):\n",
    "    api = HfApi()\n",
    "    model_info = api.model_info(model_name)\n",
    "    return {\n",
    "        \"primary_task\": model_info.pipeline_tag,\n",
    "        \"all_tags\": model_info.tags\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "info = get_model_tasks(\"google/gemma-3-27b-it\")\n",
    "print(info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e66fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 7 HuggingFace model(s)...\n",
      "==================================================\n",
      "Testing: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-14B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-14B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-3B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-0.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-72B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "==================================================\n",
      "Summary:\n",
      "  Total tested: 7\n",
      "  Supported: 4\n",
      "  Correct answers: 4\n",
      "\n",
      "✅ Working models:\n",
      "  - Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "  - Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "  - Qwen/Qwen2.5-Coder-3B-Instruct\n",
      "  - Qwen/Qwen2.5-72B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "supported",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "correct_answer",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "error",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "088a48e9-630a-40a3-a152-b6c23dcc490a",
       "rows": [
        [
         "Qwen/Qwen2.5-Coder-32B-Instruct",
         "True",
         "2",
         "True",
         null
        ],
        [
         "Qwen/Qwen2.5-Coder-14B-Instruct",
         "False",
         "",
         "False",
         "Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-14B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}"
        ],
        [
         "Qwen/Qwen2.5-Coder-7B-Instruct",
         "True",
         "2",
         "True",
         null
        ],
        [
         "Qwen/Qwen2.5-Coder-3B-Instruct",
         "True",
         "2",
         "True",
         null
        ],
        [
         "Qwen/Qwen2.5-Coder-1.5B-Instruct",
         "False",
         "",
         "False",
         "Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}"
        ],
        [
         "Qwen/Qwen2.5-Coder-0.5B-Instruct",
         "False",
         "",
         "False",
         "Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-0.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}"
        ],
        [
         "Qwen/Qwen2.5-72B-Instruct",
         "True",
         "2",
         "True",
         null
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supported</th>\n",
       "      <th>response</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-Coder-32B-Instruct</th>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-Coder-14B-Instruct</th>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>Model not supported: Error code: 400 - {'error...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-Coder-7B-Instruct</th>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-Coder-3B-Instruct</th>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-Coder-1.5B-Instruct</th>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>Model not supported: Error code: 400 - {'error...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-Coder-0.5B-Instruct</th>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>Model not supported: Error code: 400 - {'error...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-72B-Instruct</th>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  supported response  correct_answer  \\\n",
       "Qwen/Qwen2.5-Coder-32B-Instruct        True        2            True   \n",
       "Qwen/Qwen2.5-Coder-14B-Instruct       False                    False   \n",
       "Qwen/Qwen2.5-Coder-7B-Instruct         True        2            True   \n",
       "Qwen/Qwen2.5-Coder-3B-Instruct         True        2            True   \n",
       "Qwen/Qwen2.5-Coder-1.5B-Instruct      False                    False   \n",
       "Qwen/Qwen2.5-Coder-0.5B-Instruct      False                    False   \n",
       "Qwen/Qwen2.5-72B-Instruct              True        2            True   \n",
       "\n",
       "                                                                              error  \n",
       "Qwen/Qwen2.5-Coder-32B-Instruct                                                None  \n",
       "Qwen/Qwen2.5-Coder-14B-Instruct   Model not supported: Error code: 400 - {'error...  \n",
       "Qwen/Qwen2.5-Coder-7B-Instruct                                                 None  \n",
       "Qwen/Qwen2.5-Coder-3B-Instruct                                                 None  \n",
       "Qwen/Qwen2.5-Coder-1.5B-Instruct  Model not supported: Error code: 400 - {'error...  \n",
       "Qwen/Qwen2.5-Coder-0.5B-Instruct  Model not supported: Error code: 400 - {'error...  \n",
       "Qwen/Qwen2.5-72B-Instruct                                                      None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_eca6b_row0_col0, #T_eca6b_row0_col1, #T_eca6b_row0_col2, #T_eca6b_row0_col3, #T_eca6b_row1_col0, #T_eca6b_row1_col1, #T_eca6b_row1_col2, #T_eca6b_row1_col3, #T_eca6b_row2_col0, #T_eca6b_row2_col1, #T_eca6b_row2_col2, #T_eca6b_row2_col3, #T_eca6b_row3_col0, #T_eca6b_row3_col1, #T_eca6b_row3_col2, #T_eca6b_row3_col3, #T_eca6b_row4_col0, #T_eca6b_row4_col1, #T_eca6b_row4_col2, #T_eca6b_row4_col3, #T_eca6b_row5_col0, #T_eca6b_row5_col1, #T_eca6b_row5_col2, #T_eca6b_row5_col3, #T_eca6b_row6_col0, #T_eca6b_row6_col1, #T_eca6b_row6_col2, #T_eca6b_row6_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_eca6b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_eca6b_level0_col0\" class=\"col_heading level0 col0\" >supported</th>\n",
       "      <th id=\"T_eca6b_level0_col1\" class=\"col_heading level0 col1\" >response</th>\n",
       "      <th id=\"T_eca6b_level0_col2\" class=\"col_heading level0 col2\" >correct_answer</th>\n",
       "      <th id=\"T_eca6b_level0_col3\" class=\"col_heading level0 col3\" >error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_eca6b_level0_row0\" class=\"row_heading level0 row0\" >Qwen/Qwen2.5-Coder-32B-Instruct</th>\n",
       "      <td id=\"T_eca6b_row0_col0\" class=\"data row0 col0\" >True</td>\n",
       "      <td id=\"T_eca6b_row0_col1\" class=\"data row0 col1\" >2</td>\n",
       "      <td id=\"T_eca6b_row0_col2\" class=\"data row0 col2\" >True</td>\n",
       "      <td id=\"T_eca6b_row0_col3\" class=\"data row0 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eca6b_level0_row1\" class=\"row_heading level0 row1\" >Qwen/Qwen2.5-Coder-14B-Instruct</th>\n",
       "      <td id=\"T_eca6b_row1_col0\" class=\"data row1 col0\" >False</td>\n",
       "      <td id=\"T_eca6b_row1_col1\" class=\"data row1 col1\" ></td>\n",
       "      <td id=\"T_eca6b_row1_col2\" class=\"data row1 col2\" >False</td>\n",
       "      <td id=\"T_eca6b_row1_col3\" class=\"data row1 col3\" >Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-14B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eca6b_level0_row2\" class=\"row_heading level0 row2\" >Qwen/Qwen2.5-Coder-7B-Instruct</th>\n",
       "      <td id=\"T_eca6b_row2_col0\" class=\"data row2 col0\" >True</td>\n",
       "      <td id=\"T_eca6b_row2_col1\" class=\"data row2 col1\" >2</td>\n",
       "      <td id=\"T_eca6b_row2_col2\" class=\"data row2 col2\" >True</td>\n",
       "      <td id=\"T_eca6b_row2_col3\" class=\"data row2 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eca6b_level0_row3\" class=\"row_heading level0 row3\" >Qwen/Qwen2.5-Coder-3B-Instruct</th>\n",
       "      <td id=\"T_eca6b_row3_col0\" class=\"data row3 col0\" >True</td>\n",
       "      <td id=\"T_eca6b_row3_col1\" class=\"data row3 col1\" >2</td>\n",
       "      <td id=\"T_eca6b_row3_col2\" class=\"data row3 col2\" >True</td>\n",
       "      <td id=\"T_eca6b_row3_col3\" class=\"data row3 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eca6b_level0_row4\" class=\"row_heading level0 row4\" >Qwen/Qwen2.5-Coder-1.5B-Instruct</th>\n",
       "      <td id=\"T_eca6b_row4_col0\" class=\"data row4 col0\" >False</td>\n",
       "      <td id=\"T_eca6b_row4_col1\" class=\"data row4 col1\" ></td>\n",
       "      <td id=\"T_eca6b_row4_col2\" class=\"data row4 col2\" >False</td>\n",
       "      <td id=\"T_eca6b_row4_col3\" class=\"data row4 col3\" >Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eca6b_level0_row5\" class=\"row_heading level0 row5\" >Qwen/Qwen2.5-Coder-0.5B-Instruct</th>\n",
       "      <td id=\"T_eca6b_row5_col0\" class=\"data row5 col0\" >False</td>\n",
       "      <td id=\"T_eca6b_row5_col1\" class=\"data row5 col1\" ></td>\n",
       "      <td id=\"T_eca6b_row5_col2\" class=\"data row5 col2\" >False</td>\n",
       "      <td id=\"T_eca6b_row5_col3\" class=\"data row5 col3\" >Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-0.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eca6b_level0_row6\" class=\"row_heading level0 row6\" >Qwen/Qwen2.5-72B-Instruct</th>\n",
       "      <td id=\"T_eca6b_row6_col0\" class=\"data row6 col0\" >True</td>\n",
       "      <td id=\"T_eca6b_row6_col1\" class=\"data row6 col1\" >2</td>\n",
       "      <td id=\"T_eca6b_row6_col2\" class=\"data row6 col2\" >True</td>\n",
       "      <td id=\"T_eca6b_row6_col3\" class=\"data row6 col3\" >None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27ae56d4f70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_test = [\n",
    "    \"huggingface/zai-org/GLM-4.5\",\n",
    "    \"huggingface/zai-org/GLM-4.5-Air\", \n",
    "    \"huggingface/nvidia/Llama-3.1-Nemotron-Nano-8B-v1\", \n",
    "    \"huggingface/meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    \"huggingface/openai/gpt-oss-120b\", \n",
    "    \"huggingface/openai/gpt-oss-20b\", \n",
    "    \"huggingface/nvidia/Mistral-NeMo-Minitron-8B-Instruct\", \n",
    "    \"huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \n",
    "    \"huggingface/tiiuae/falcon-7b-instruct\", \n",
    "    \"huggingface/epfl-llm/meditron-70b\", \n",
    "    \"huggingface/epfl-llm/meditron-7b\",\n",
    "    \"huggingface/Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    \"huggingface/Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    \"huggingface/Qwen/Qwen2.5-Coder-3B-Instruct\",\n",
    "    \"HuggingFaceTB/SmolLM3-3B\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "]\n",
    "model_to_test = [\"huggingface/Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "                 \"huggingface/Qwen/Qwen2.5-Coder-14B-Instruct\",\n",
    "                 \"huggingface/Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "                 \"huggingface/Qwen/Qwen2.5-Coder-3B-Instruct\",\n",
    "                 \"huggingface/Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "                 \"huggingface/Qwen/Qwen2.5-Coder-0.5B-Instruct\",\n",
    "                 \"huggingface/Qwen/Qwen2.5-72B-Instruct\"\n",
    "]\n",
    "import pandas as pd\n",
    "\n",
    "# Run test\n",
    "result = check_model_supported(model_to_test, max_tokens=100)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(result['detailed_results'], orient='index')\n",
    "\n",
    "# Display the table\n",
    "display(df)\n",
    "\n",
    "# Or with better formatting\n",
    "df.style.set_properties(**{'text-align': 'left'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da2f0dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "=== Analyzing HuggingFaceTB ===\n",
      "Fetching HuggingFaceTB text-generation models...\n",
      "Checking HuggingFaceTB/SmolLM3-3B-checkpoints: is_instruct=False, downloads=1666, private=False\n",
      "Checking HuggingFaceTB/SmolLM3-3B-Base: is_instruct=False, downloads=9625, private=False\n",
      "Checking HuggingFaceTB/SmolLM3-3B: is_instruct=True, downloads=586058, private=False\n",
      "Checking HuggingFaceTB/smollm2-135M-SFT-Only: is_instruct=False, downloads=49, private=False\n",
      "Checking HuggingFaceTB/SmolLM3-3B-ONNX: is_instruct=True, downloads=436, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-360M-Instruct: is_instruct=True, downloads=526226, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-135M-Instruct: is_instruct=True, downloads=390272, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-1.7B-Instruct: is_instruct=True, downloads=47671, private=False\n",
      "Checking HuggingFaceTB/SmolVLM2-2.2B-Base: is_instruct=True, downloads=960, private=False\n",
      "Checking HuggingFaceTB/SmolVLM-256M-Instruct: is_instruct=True, downloads=510242, private=False\n",
      "Checking HuggingFaceTB/SmolVLM-Instruct: is_instruct=True, downloads=79620, private=False\n",
      "Checking HuggingFaceTB/SmolVLM-500M-Instruct: is_instruct=True, downloads=38117, private=False\n",
      "Checking HuggingFaceTB/SmolVLM2-256M-Video-Instruct: is_instruct=True, downloads=123138, private=False\n",
      "Checking HuggingFaceTB/SmolVLM2-500M-Video-Instruct: is_instruct=True, downloads=64055, private=False\n",
      "Checking HuggingFaceTB/SmolVLM2-2.2B-Instruct: is_instruct=True, downloads=122121, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-360M-intermediate-checkpoints: is_instruct=False, downloads=8, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints: is_instruct=False, downloads=140, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-135M-intermediate-checkpoints: is_instruct=False, downloads=4, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-1.7B-Instruct-16k: is_instruct=True, downloads=7, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-python: is_instruct=False, downloads=27, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-cpp: is_instruct=False, downloads=7, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-javascript: is_instruct=False, downloads=12, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-java: is_instruct=False, downloads=134, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-c: is_instruct=False, downloads=4, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-sql: is_instruct=False, downloads=6, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-rust: is_instruct=False, downloads=9, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-go: is_instruct=False, downloads=7, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-markdown: is_instruct=False, downloads=5, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-shell: is_instruct=False, downloads=9, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-swift: is_instruct=False, downloads=7, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-csharp: is_instruct=False, downloads=4, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-php: is_instruct=False, downloads=5, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-typescript: is_instruct=False, downloads=9, private=False\n",
      "Checking HuggingFaceTB/stack-edu-classifier-ruby: is_instruct=False, downloads=6, private=False\n",
      "Checking HuggingFaceTB/smolvlm-app-config: is_instruct=False, downloads=1587, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-135M: is_instruct=False, downloads=954096, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-360M: is_instruct=False, downloads=18576, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-1.7B: is_instruct=False, downloads=13737, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-1.7B-sft-only: is_instruct=False, downloads=3, private=False\n",
      "Checking HuggingFaceTB/SmolVLM-500M-Base: is_instruct=False, downloads=1039, private=False\n",
      "Checking HuggingFaceTB/SmolVLM-256M-Base: is_instruct=False, downloads=7625, private=False\n",
      "Checking HuggingFaceTB/FineMath-Llama-3B: is_instruct=False, downloads=60, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-3plus-160B: is_instruct=False, downloads=6, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-4plus-160B: is_instruct=False, downloads=3, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-finemath-infimath-3plus: is_instruct=False, downloads=3, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-finemath-infimath-4plus: is_instruct=False, downloads=10, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-infiwebmath: is_instruct=False, downloads=2, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-fwedu: is_instruct=False, downloads=6, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-infiwebmath-3plus: is_instruct=False, downloads=2, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-owm: is_instruct=False, downloads=3, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-finemath-4plus: is_instruct=False, downloads=3, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-finemath-3plus: is_instruct=False, downloads=4, private=False\n",
      "Checking HuggingFaceTB/finemath-ablation-infiwebmath-4plus: is_instruct=False, downloads=4, private=False\n",
      "Checking HuggingFaceTB/finemath-classifier: is_instruct=False, downloads=12090, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-nanotron-ckpt: is_instruct=False, downloads=0, private=False\n",
      "Checking HuggingFaceTB/SmolVLM-Base: is_instruct=True, downloads=8159, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-1.7B-Instruct-Q8-mlx: is_instruct=True, downloads=12, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-135M-Instruct-Q8-mlx: is_instruct=True, downloads=37, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-360M-Instruct-Q8-mlx: is_instruct=True, downloads=5, private=False\n",
      "Checking HuggingFaceTB/SmolVLM-Synthetic: is_instruct=True, downloads=218, private=False\n",
      "Checking HuggingFaceTB/SmolVLM-Instruct-DPO: is_instruct=True, downloads=299, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF: is_instruct=True, downloads=2703, private=False\n",
      "Checking HuggingFaceTB/SmolLM2-360M-Instruct-GGUF: is_instruct=True, downloads=3016, private=False\n",
      "Checking HuggingFaceTB/SmolLM-1.7B: is_instruct=False, downloads=142450, private=False\n",
      "Checking HuggingFaceTB/SmolLM-135M-Instruct: is_instruct=True, downloads=10086, private=False\n",
      "Checking HuggingFaceTB/cosmo2-tokenizer: is_instruct=False, downloads=0, private=False\n",
      "Checking HuggingFaceTB/SmolLM-1.7B-Instruct: is_instruct=True, downloads=3941, private=False\n",
      "Checking HuggingFaceTB/SmolLM-360M-Instruct: is_instruct=True, downloads=6630, private=False\n",
      "Checking HuggingFaceTB/smollm-135M-instruct-v0.2-Q8_0-GGUF: is_instruct=True, downloads=57, private=False\n",
      "Checking HuggingFaceTB/smollm-360M-instruct-add-basics-q0f16-MLC: is_instruct=True, downloads=281, private=False\n",
      "Checking HuggingFaceTB/smollm-1.7B-instruct-add-basics-q4f16_1-MLC: is_instruct=True, downloads=3, private=False\n",
      "Checking HuggingFaceTB/smollm-135M-instruct-add-basics-q0f16-MLC: is_instruct=True, downloads=1, private=False\n",
      "Checking HuggingFaceTB/smollm-360M-instruct-add-basics: is_instruct=True, downloads=17, private=False\n",
      "Checking HuggingFaceTB/SmolLM-360M-Instruct-ONNX-fp16: is_instruct=True, downloads=944, private=False\n",
      "Checking HuggingFaceTB/smollm-360M-instruct-v0.2-Q8_0-GGUF: is_instruct=True, downloads=213, private=False\n",
      "Checking HuggingFaceTB/smollm-1.7B-instruct-v0.2-Q8_0-GGUF: is_instruct=True, downloads=16, private=False\n",
      "Checking HuggingFaceTB/SmolLM-360M: is_instruct=False, downloads=123477, private=False\n",
      "Checking HuggingFaceTB/SmolLM-135M: is_instruct=False, downloads=267242, private=False\n",
      "Checking HuggingFaceTB/python-edu-scorer: is_instruct=False, downloads=31, private=False\n",
      "Checking HuggingFaceTB/cosmo-1b: is_instruct=False, downloads=408, private=False\n",
      "Found 31 HuggingFaceTB models to analyze\n",
      "Testing 31 models for HuggingFace support...\n",
      "Testing 31 HuggingFace model(s)...\n",
      "==================================================\n",
      "Testing: HuggingFaceTB/SmolLM3-3B-ONNX\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM3-3B-ONNX' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM3-3B\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "Okay, the user is asking for the sum of 1 and 1. Let me just add those numbers together. 1 plus 1 equals 2. That's straightforward. I need to make sure I don't include any extra steps or explanations. The answer is simply 2. Alright, ready to provide the result.\n",
      "</think>\n",
      "\n",
      "2' (CORRECT ✓)\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM2-2.2B-Base\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM2-2.2B-Base' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-1.7B-Instruct-16k\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-1.7B-Instruct-16k' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM2-500M-Video-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM2-256M-Video-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM2-256M-Video-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM2-2.2B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM2-2.2B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM-500M-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM-500M-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM-256M-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM-256M-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-135M-Instruct-Q8-mlx\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-135M-Instruct-Q8-mlx' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-360M-Instruct-Q8-mlx\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-360M-Instruct-Q8-mlx' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-1.7B-Instruct-Q8-mlx\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-1.7B-Instruct-Q8-mlx' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM-Instruct-DPO\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM-Instruct-DPO' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM-Synthetic\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM-Synthetic' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM-Base\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM-Base' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolVLM-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolVLM-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-360M-Instruct-GGUF\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-360M-Instruct-GGUF' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-1.7B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-360M-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-360M-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-135M-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/smollm-1.7B-instruct-add-basics-q4f16_1-MLC\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/smollm-1.7B-instruct-add-basics-q4f16_1-MLC' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM-360M-Instruct-ONNX-fp16\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM-360M-Instruct-ONNX-fp16' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/smollm-1.7B-instruct-v0.2-Q8_0-GGUF\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/smollm-1.7B-instruct-v0.2-Q8_0-GGUF' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/smollm-360M-instruct-add-basics-q0f16-MLC\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/smollm-360M-instruct-add-basics-q0f16-MLC' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/smollm-135M-instruct-v0.2-Q8_0-GGUF\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/smollm-135M-instruct-v0.2-Q8_0-GGUF' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/smollm-360M-instruct-v0.2-Q8_0-GGUF\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/smollm-360M-instruct-v0.2-Q8_0-GGUF' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/smollm-360M-instruct-add-basics\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/smollm-360M-instruct-add-basics' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM-1.7B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM-1.7B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM-135M-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM-135M-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM-360M-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM-360M-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "==================================================\n",
      "Summary:\n",
      "  Total tested: 31\n",
      "  Supported: 1\n",
      "  Correct answers: 1\n",
      "\n",
      "✅ Working models:\n",
      "  - HuggingFaceTB/SmolLM3-3B\n",
      "HuggingFaceTB Analysis Summary:\n",
      "  Total models: 31\n",
      "  Supported models: 1\n",
      "  Models with known parameters: 11\n",
      "\n",
      "Working HuggingFaceTB models (1):\n",
      "  - SmolLM3-3B (3B, 586,058 downloads)\n",
      "--------------------------------------------------\n",
      "\n",
      "=== COMBINED ANALYSIS ===\n",
      "Total models analyzed: 31\n",
      "Total supported models: 1\n",
      "\n",
      "Support rate by author:\n",
      "               Total_Models  Supported_Models Support_Rate\n",
      "author                                                    \n",
      "HuggingFaceTB            31                 1         3.2%\n",
      "\n",
      "Top 10 working models (by downloads):\n",
      "  HuggingFaceTB/SmolLM3-3B (3B, 586,058 downloads)\n",
      "\n",
      "Combined DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "parameters",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "released",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "downloads",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "supported",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "f5aba0ee-5947-4c7c-873c-3b4033f9825f",
       "rows": [
        [
         "0",
         "HuggingFaceTB",
         "SmolLM3-3B",
         "https://huggingface.co/HuggingFaceTB/SmolLM3-3B",
         "3000000000.0",
         "2025-07-08",
         "586058",
         "True"
        ],
        [
         "1",
         "HuggingFaceTB",
         "SmolLM3-3B-ONNX",
         "https://huggingface.co/HuggingFaceTB/SmolLM3-3B-ONNX",
         "3000000000.0",
         "2025-07-08",
         "436",
         "False"
        ],
        [
         "2",
         "HuggingFaceTB",
         "SmolVLM2-2.2B-Base",
         "https://huggingface.co/HuggingFaceTB/SmolVLM2-2.2B-Base",
         "2200000000.0",
         "2025-04-14",
         "960",
         "False"
        ],
        [
         "3",
         "HuggingFaceTB",
         "SmolLM2-1.7B-Instruct-16k",
         "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-16k",
         "1700000000.0",
         "2025-02-21",
         "7",
         "False"
        ],
        [
         "4",
         "HuggingFaceTB",
         "SmolVLM2-256M-Video-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolVLM2-256M-Video-Instruct",
         null,
         "2025-02-11",
         "123138",
         "False"
        ],
        [
         "5",
         "HuggingFaceTB",
         "SmolVLM2-500M-Video-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolVLM2-500M-Video-Instruct",
         null,
         "2025-02-11",
         "64055",
         "False"
        ],
        [
         "6",
         "HuggingFaceTB",
         "SmolVLM2-2.2B-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolVLM2-2.2B-Instruct",
         "2200000000.0",
         "2025-02-08",
         "122121",
         "False"
        ],
        [
         "7",
         "HuggingFaceTB",
         "SmolVLM-500M-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolVLM-500M-Instruct",
         null,
         "2025-01-20",
         "38117",
         "False"
        ],
        [
         "8",
         "HuggingFaceTB",
         "SmolVLM-256M-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct",
         null,
         "2025-01-17",
         "510242",
         "False"
        ],
        [
         "9",
         "HuggingFaceTB",
         "SmolLM2-135M-Instruct-Q8-mlx",
         "https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct-Q8-mlx",
         null,
         "2024-11-27",
         "37",
         "False"
        ],
        [
         "10",
         "HuggingFaceTB",
         "SmolLM2-1.7B-Instruct-Q8-mlx",
         "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-Q8-mlx",
         "1700000000.0",
         "2024-11-27",
         "12",
         "False"
        ],
        [
         "11",
         "HuggingFaceTB",
         "SmolLM2-360M-Instruct-Q8-mlx",
         "https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-Q8-mlx",
         null,
         "2024-11-27",
         "5",
         "False"
        ],
        [
         "12",
         "HuggingFaceTB",
         "SmolVLM-Instruct-DPO",
         "https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct-DPO",
         null,
         "2024-11-26",
         "299",
         "False"
        ],
        [
         "13",
         "HuggingFaceTB",
         "SmolVLM-Base",
         "https://huggingface.co/HuggingFaceTB/SmolVLM-Base",
         null,
         "2024-11-22",
         "8159",
         "False"
        ],
        [
         "14",
         "HuggingFaceTB",
         "SmolVLM-Synthetic",
         "https://huggingface.co/HuggingFaceTB/SmolVLM-Synthetic",
         null,
         "2024-11-22",
         "218",
         "False"
        ],
        [
         "15",
         "HuggingFaceTB",
         "SmolVLM-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct",
         null,
         "2024-11-18",
         "79620",
         "False"
        ],
        [
         "16",
         "HuggingFaceTB",
         "SmolLM2-360M-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct",
         null,
         "2024-10-31",
         "526226",
         "False"
        ],
        [
         "17",
         "HuggingFaceTB",
         "SmolLM2-135M-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct",
         null,
         "2024-10-31",
         "390272",
         "False"
        ],
        [
         "18",
         "HuggingFaceTB",
         "SmolLM2-1.7B-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct",
         "1700000000.0",
         "2024-10-31",
         "47671",
         "False"
        ],
        [
         "19",
         "HuggingFaceTB",
         "SmolLM2-360M-Instruct-GGUF",
         "https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF",
         null,
         "2024-10-31",
         "3016",
         "False"
        ],
        [
         "20",
         "HuggingFaceTB",
         "SmolLM2-1.7B-Instruct-GGUF",
         "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF",
         "1700000000.0",
         "2024-10-31",
         "2703",
         "False"
        ],
        [
         "21",
         "HuggingFaceTB",
         "smollm-1.7B-instruct-add-basics-q4f16_1-MLC",
         "https://huggingface.co/HuggingFaceTB/smollm-1.7B-instruct-add-basics-q4f16_1-MLC",
         "1700000000.0",
         "2024-08-18",
         "3",
         "False"
        ],
        [
         "22",
         "HuggingFaceTB",
         "SmolLM-360M-Instruct-ONNX-fp16",
         "https://huggingface.co/HuggingFaceTB/SmolLM-360M-Instruct-ONNX-fp16",
         null,
         "2024-08-17",
         "944",
         "False"
        ],
        [
         "23",
         "HuggingFaceTB",
         "smollm-1.7B-instruct-v0.2-Q8_0-GGUF",
         "https://huggingface.co/HuggingFaceTB/smollm-1.7B-instruct-v0.2-Q8_0-GGUF",
         "1700000000.0",
         "2024-08-17",
         "16",
         "False"
        ],
        [
         "24",
         "HuggingFaceTB",
         "smollm-360M-instruct-add-basics-q0f16-MLC",
         "https://huggingface.co/HuggingFaceTB/smollm-360M-instruct-add-basics-q0f16-MLC",
         null,
         "2024-08-16",
         "281",
         "False"
        ],
        [
         "25",
         "HuggingFaceTB",
         "smollm-135M-instruct-v0.2-Q8_0-GGUF",
         "https://huggingface.co/HuggingFaceTB/smollm-135M-instruct-v0.2-Q8_0-GGUF",
         null,
         "2024-08-14",
         "57",
         "False"
        ],
        [
         "26",
         "HuggingFaceTB",
         "smollm-360M-instruct-v0.2-Q8_0-GGUF",
         "https://huggingface.co/HuggingFaceTB/smollm-360M-instruct-v0.2-Q8_0-GGUF",
         null,
         "2024-08-13",
         "213",
         "False"
        ],
        [
         "27",
         "HuggingFaceTB",
         "smollm-360M-instruct-add-basics",
         "https://huggingface.co/HuggingFaceTB/smollm-360M-instruct-add-basics",
         null,
         "2024-08-13",
         "17",
         "False"
        ],
        [
         "28",
         "HuggingFaceTB",
         "SmolLM-135M-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct",
         null,
         "2024-07-15",
         "10086",
         "False"
        ],
        [
         "29",
         "HuggingFaceTB",
         "SmolLM-360M-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolLM-360M-Instruct",
         null,
         "2024-07-15",
         "6630",
         "False"
        ],
        [
         "30",
         "HuggingFaceTB",
         "SmolLM-1.7B-Instruct",
         "https://huggingface.co/HuggingFaceTB/SmolLM-1.7B-Instruct",
         "1700000000.0",
         "2024-07-15",
         "3941",
         "False"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 31
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>model_name</th>\n",
       "      <th>url</th>\n",
       "      <th>parameters</th>\n",
       "      <th>released</th>\n",
       "      <th>downloads</th>\n",
       "      <th>supported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM3-3B</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM3-3B</td>\n",
       "      <td>3.000000e+09</td>\n",
       "      <td>2025-07-08</td>\n",
       "      <td>586058</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM3-3B-ONNX</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM3-3...</td>\n",
       "      <td>3.000000e+09</td>\n",
       "      <td>2025-07-08</td>\n",
       "      <td>436</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM2-2.2B-Base</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM2-...</td>\n",
       "      <td>2.200000e+09</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>960</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM2-1.7B-Instruct-16k</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM2-1...</td>\n",
       "      <td>1.700000e+09</td>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM2-256M-Video-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM2-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-11</td>\n",
       "      <td>123138</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM2-500M-Video-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM2-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-11</td>\n",
       "      <td>64055</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM2-2.2B-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM2-...</td>\n",
       "      <td>2.200000e+09</td>\n",
       "      <td>2025-02-08</td>\n",
       "      <td>122121</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM-500M-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM-5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-20</td>\n",
       "      <td>38117</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM-256M-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-17</td>\n",
       "      <td>510242</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM2-135M-Instruct-Q8-mlx</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM2-1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-27</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM2-1.7B-Instruct-Q8-mlx</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM2-1...</td>\n",
       "      <td>1.700000e+09</td>\n",
       "      <td>2024-11-27</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM2-360M-Instruct-Q8-mlx</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM2-3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-27</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM-Instruct-DPO</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM-I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-26</td>\n",
       "      <td>299</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM-Base</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM-Base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-22</td>\n",
       "      <td>8159</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM-Synthetic</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM-S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-22</td>\n",
       "      <td>218</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolVLM-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolVLM-I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>79620</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM2-360M-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM2-3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>526226</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM2-135M-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM2-1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>390272</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM2-1.7B-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM2-1...</td>\n",
       "      <td>1.700000e+09</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>47671</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM2-360M-Instruct-GGUF</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM2-3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>3016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM2-1.7B-Instruct-GGUF</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM2-1...</td>\n",
       "      <td>1.700000e+09</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>2703</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>smollm-1.7B-instruct-add-basics-q4f16_1-MLC</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/smollm-1....</td>\n",
       "      <td>1.700000e+09</td>\n",
       "      <td>2024-08-18</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM-360M-Instruct-ONNX-fp16</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM-36...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-17</td>\n",
       "      <td>944</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>smollm-1.7B-instruct-v0.2-Q8_0-GGUF</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/smollm-1....</td>\n",
       "      <td>1.700000e+09</td>\n",
       "      <td>2024-08-17</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>smollm-360M-instruct-add-basics-q0f16-MLC</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/smollm-36...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-16</td>\n",
       "      <td>281</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>smollm-135M-instruct-v0.2-Q8_0-GGUF</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/smollm-13...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-14</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>smollm-360M-instruct-v0.2-Q8_0-GGUF</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/smollm-36...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-13</td>\n",
       "      <td>213</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>smollm-360M-instruct-add-basics</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/smollm-36...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-13</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM-135M-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM-13...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>10086</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM-360M-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM-36...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>6630</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>HuggingFaceTB</td>\n",
       "      <td>SmolLM-1.7B-Instruct</td>\n",
       "      <td>https://huggingface.co/HuggingFaceTB/SmolLM-1....</td>\n",
       "      <td>1.700000e+09</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>3941</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                   model_name  \\\n",
       "0   HuggingFaceTB                                   SmolLM3-3B   \n",
       "1   HuggingFaceTB                              SmolLM3-3B-ONNX   \n",
       "2   HuggingFaceTB                           SmolVLM2-2.2B-Base   \n",
       "3   HuggingFaceTB                    SmolLM2-1.7B-Instruct-16k   \n",
       "4   HuggingFaceTB                 SmolVLM2-256M-Video-Instruct   \n",
       "5   HuggingFaceTB                 SmolVLM2-500M-Video-Instruct   \n",
       "6   HuggingFaceTB                       SmolVLM2-2.2B-Instruct   \n",
       "7   HuggingFaceTB                        SmolVLM-500M-Instruct   \n",
       "8   HuggingFaceTB                        SmolVLM-256M-Instruct   \n",
       "9   HuggingFaceTB                 SmolLM2-135M-Instruct-Q8-mlx   \n",
       "10  HuggingFaceTB                 SmolLM2-1.7B-Instruct-Q8-mlx   \n",
       "11  HuggingFaceTB                 SmolLM2-360M-Instruct-Q8-mlx   \n",
       "12  HuggingFaceTB                         SmolVLM-Instruct-DPO   \n",
       "13  HuggingFaceTB                                 SmolVLM-Base   \n",
       "14  HuggingFaceTB                            SmolVLM-Synthetic   \n",
       "15  HuggingFaceTB                             SmolVLM-Instruct   \n",
       "16  HuggingFaceTB                        SmolLM2-360M-Instruct   \n",
       "17  HuggingFaceTB                        SmolLM2-135M-Instruct   \n",
       "18  HuggingFaceTB                        SmolLM2-1.7B-Instruct   \n",
       "19  HuggingFaceTB                   SmolLM2-360M-Instruct-GGUF   \n",
       "20  HuggingFaceTB                   SmolLM2-1.7B-Instruct-GGUF   \n",
       "21  HuggingFaceTB  smollm-1.7B-instruct-add-basics-q4f16_1-MLC   \n",
       "22  HuggingFaceTB               SmolLM-360M-Instruct-ONNX-fp16   \n",
       "23  HuggingFaceTB          smollm-1.7B-instruct-v0.2-Q8_0-GGUF   \n",
       "24  HuggingFaceTB    smollm-360M-instruct-add-basics-q0f16-MLC   \n",
       "25  HuggingFaceTB          smollm-135M-instruct-v0.2-Q8_0-GGUF   \n",
       "26  HuggingFaceTB          smollm-360M-instruct-v0.2-Q8_0-GGUF   \n",
       "27  HuggingFaceTB              smollm-360M-instruct-add-basics   \n",
       "28  HuggingFaceTB                         SmolLM-135M-Instruct   \n",
       "29  HuggingFaceTB                         SmolLM-360M-Instruct   \n",
       "30  HuggingFaceTB                         SmolLM-1.7B-Instruct   \n",
       "\n",
       "                                                  url    parameters  \\\n",
       "0     https://huggingface.co/HuggingFaceTB/SmolLM3-3B  3.000000e+09   \n",
       "1   https://huggingface.co/HuggingFaceTB/SmolLM3-3...  3.000000e+09   \n",
       "2   https://huggingface.co/HuggingFaceTB/SmolVLM2-...  2.200000e+09   \n",
       "3   https://huggingface.co/HuggingFaceTB/SmolLM2-1...  1.700000e+09   \n",
       "4   https://huggingface.co/HuggingFaceTB/SmolVLM2-...           NaN   \n",
       "5   https://huggingface.co/HuggingFaceTB/SmolVLM2-...           NaN   \n",
       "6   https://huggingface.co/HuggingFaceTB/SmolVLM2-...  2.200000e+09   \n",
       "7   https://huggingface.co/HuggingFaceTB/SmolVLM-5...           NaN   \n",
       "8   https://huggingface.co/HuggingFaceTB/SmolVLM-2...           NaN   \n",
       "9   https://huggingface.co/HuggingFaceTB/SmolLM2-1...           NaN   \n",
       "10  https://huggingface.co/HuggingFaceTB/SmolLM2-1...  1.700000e+09   \n",
       "11  https://huggingface.co/HuggingFaceTB/SmolLM2-3...           NaN   \n",
       "12  https://huggingface.co/HuggingFaceTB/SmolVLM-I...           NaN   \n",
       "13  https://huggingface.co/HuggingFaceTB/SmolVLM-Base           NaN   \n",
       "14  https://huggingface.co/HuggingFaceTB/SmolVLM-S...           NaN   \n",
       "15  https://huggingface.co/HuggingFaceTB/SmolVLM-I...           NaN   \n",
       "16  https://huggingface.co/HuggingFaceTB/SmolLM2-3...           NaN   \n",
       "17  https://huggingface.co/HuggingFaceTB/SmolLM2-1...           NaN   \n",
       "18  https://huggingface.co/HuggingFaceTB/SmolLM2-1...  1.700000e+09   \n",
       "19  https://huggingface.co/HuggingFaceTB/SmolLM2-3...           NaN   \n",
       "20  https://huggingface.co/HuggingFaceTB/SmolLM2-1...  1.700000e+09   \n",
       "21  https://huggingface.co/HuggingFaceTB/smollm-1....  1.700000e+09   \n",
       "22  https://huggingface.co/HuggingFaceTB/SmolLM-36...           NaN   \n",
       "23  https://huggingface.co/HuggingFaceTB/smollm-1....  1.700000e+09   \n",
       "24  https://huggingface.co/HuggingFaceTB/smollm-36...           NaN   \n",
       "25  https://huggingface.co/HuggingFaceTB/smollm-13...           NaN   \n",
       "26  https://huggingface.co/HuggingFaceTB/smollm-36...           NaN   \n",
       "27  https://huggingface.co/HuggingFaceTB/smollm-36...           NaN   \n",
       "28  https://huggingface.co/HuggingFaceTB/SmolLM-13...           NaN   \n",
       "29  https://huggingface.co/HuggingFaceTB/SmolLM-36...           NaN   \n",
       "30  https://huggingface.co/HuggingFaceTB/SmolLM-1....  1.700000e+09   \n",
       "\n",
       "      released  downloads  supported  \n",
       "0   2025-07-08     586058       True  \n",
       "1   2025-07-08        436      False  \n",
       "2   2025-04-14        960      False  \n",
       "3   2025-02-21          7      False  \n",
       "4   2025-02-11     123138      False  \n",
       "5   2025-02-11      64055      False  \n",
       "6   2025-02-08     122121      False  \n",
       "7   2025-01-20      38117      False  \n",
       "8   2025-01-17     510242      False  \n",
       "9   2024-11-27         37      False  \n",
       "10  2024-11-27         12      False  \n",
       "11  2024-11-27          5      False  \n",
       "12  2024-11-26        299      False  \n",
       "13  2024-11-22       8159      False  \n",
       "14  2024-11-22        218      False  \n",
       "15  2024-11-18      79620      False  \n",
       "16  2024-10-31     526226      False  \n",
       "17  2024-10-31     390272      False  \n",
       "18  2024-10-31      47671      False  \n",
       "19  2024-10-31       3016      False  \n",
       "20  2024-10-31       2703      False  \n",
       "21  2024-08-18          3      False  \n",
       "22  2024-08-17        944      False  \n",
       "23  2024-08-17         16      False  \n",
       "24  2024-08-16        281      False  \n",
       "25  2024-08-14         57      False  \n",
       "26  2024-08-13        213      False  \n",
       "27  2024-08-13         17      False  \n",
       "28  2024-07-15      10086      False  \n",
       "29  2024-07-15       6630      False  \n",
       "30  2024-07-15       3941      False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Authors to analyze\n",
    "authors = [\"mistralai\", \"meta-llama\", \"google\", \"microsoft\", \"tiiuae\"]\n",
    "# authors = [\"Qwen\", \"deepseek-ai\", \"zai-org\"]\n",
    "authors = [\"HuggingFaceTB\"]\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "for author in authors:\n",
    "    print(f\"\\n=== Analyzing {author} ===\")\n",
    "    df_author = analyze_author_models(author, top_n=100)\n",
    "    \n",
    "    if not df_author.empty:\n",
    "        all_dataframes.append(df_author)\n",
    "        \n",
    "        # Show summary for this author\n",
    "        working_models = df_author[df_author['supported'] == True]\n",
    "        print(f\"\\nWorking {author} models ({len(working_models)}):\")\n",
    "        for _, row in working_models.head(5).iterrows():  # Show top 5 working\n",
    "            params_str = f\"{row['parameters']/1_000_000_000:g}B\" if pd.notna(row['parameters']) else \"Unknown\"\n",
    "            print(f\"  - {row['model_name']} ({params_str}, {row['downloads']:,} downloads)\")\n",
    "        \n",
    "        if len(working_models) > 5:\n",
    "            print(f\"  ... and {len(working_models) - 5} more\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Combine all dataframes\n",
    "if all_dataframes:\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n=== COMBINED ANALYSIS ===\")\n",
    "    print(f\"Total models analyzed: {len(combined_df)}\")\n",
    "    print(f\"Total supported models: {combined_df['supported'].sum()}\")\n",
    "    \n",
    "    # Summary by author\n",
    "    print(f\"\\nSupport rate by author:\")\n",
    "    author_summary = combined_df.groupby('author').agg({\n",
    "        'supported': ['count', 'sum', lambda x: f\"{(x.sum()/len(x)*100):.1f}%\"]\n",
    "    }).round(1)\n",
    "    author_summary.columns = ['Total_Models', 'Supported_Models', 'Support_Rate']\n",
    "    print(author_summary)\n",
    "    \n",
    "    # Show top working models across all authors\n",
    "    all_working = combined_df[combined_df['supported'] == True].sort_values('downloads', ascending=False)\n",
    "    print(f\"\\nTop 10 working models (by downloads):\")\n",
    "    for _, row in all_working.head(10).iterrows():\n",
    "        params_str = f\"{row['parameters']/1_000_000_000:g}B\" if pd.notna(row['parameters']) else \"Unknown\"\n",
    "        print(f\"  {row['author']}/{row['model_name']} ({params_str}, {row['downloads']:,} downloads)\")\n",
    "    \n",
    "    # Display the combined dataframe\n",
    "    print(f\"\\nCombined DataFrame:\")\n",
    "    display(combined_df)\n",
    "    # combined_df = combined_df[combined_df['supported'] == True]\n",
    "    combined_df.to_csv('models_analysis_' + \"+\".join(authors) + '.csv', index=False)\n",
    "    # Return the combined dataframe\n",
    "    combined_df\n",
    "else:\n",
    "    print(\"No data retrieved for any authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dd11459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1963210 models for fireworks-ai:\n",
      "  - Qwen/Qwen-Image-Edit\n",
      "  - google/gemma-3-270m\n",
      "  - tencent/Hunyuan-GameCraft-1.0\n",
      "  - deepseek-ai/DeepSeek-V3.1-Base\n",
      "  - openai/gpt-oss-20b\n",
      "  - zai-org/GLM-4.5V\n",
      "  - google/gemma-3-270m-it\n",
      "  - Qwen/Qwen-Image\n",
      "  - openai/gpt-oss-120b\n",
      "  - janhq/Jan-v1-4B\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Get models for a specific provider\n",
    "api = HfApi()\n",
    "\n",
    "# Get models with inference provider filter\n",
    "models = api.list_models(\n",
    "    limit=None  # Get all models\n",
    ")\n",
    "'''\n",
    "models = api.list_models(\n",
    "    filter=\"inference_provider:fireworks-ai\",\n",
    "    limit=None  # Get all models\n",
    ")\n",
    "'''\n",
    "# Extract just the model names\n",
    "model_names = [model.id for model in models]\n",
    "\n",
    "print(f\"Found {len(model_names)} models for fireworks-ai:\")\n",
    "for name in model_names[:10]:  # Show first 10\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd8ce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 294 Qwen text-generation models:\n",
      "\n",
      "📦 Qwen/Qwen2.5-14B-Instruct\n",
      "   ⬇️  12,195,236 downloads, ❤️  264 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-7B-Instruct\n",
      "   ⬇️  9,128,290 downloads, ❤️  760 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-Base\n",
      "   ⬇️  5,476,711 downloads, ❤️  50 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-8B\n",
      "   ⬇️  5,160,545 downloads, ❤️  544 likes\n",
      "   📅 Created: 2025-04-27\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Embedding-0.6B\n",
      "   ⬇️  3,311,960 downloads, ❤️  493 likes\n",
      "   📅 Created: 2025-06-03\n",
      "   🏷️  ['text-embeddings-inference', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-1.5B-Instruct\n",
      "   ⬇️  2,998,900 downloads, ❤️  490 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-0.6B\n",
      "   ⬇️  2,932,330 downloads, ❤️  552 likes\n",
      "   📅 Created: 2025-04-27\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-8B-Base\n",
      "   ⬇️  1,814,798 downloads, ❤️  49 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-7B-Instruct-1M\n",
      "   ⬇️  1,419,379 downloads, ❤️  346 likes\n",
      "   📅 Created: 2025-01-23\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-1.7B\n",
      "   ⬇️  1,238,114 downloads, ❤️  236 likes\n",
      "   📅 Created: 2025-04-27\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-0.6B-Base\n",
      "   ⬇️  1,225,974 downloads, ❤️  87 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-3B-Instruct\n",
      "   ⬇️  1,212,575 downloads, ❤️  290 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B\n",
      "   ⬇️  1,172,110 downloads, ❤️  366 likes\n",
      "   📅 Created: 2025-04-27\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-0.5B-Instruct\n",
      "   ⬇️  1,107,769 downloads, ❤️  357 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-7B\n",
      "   ⬇️  1,050,445 downloads, ❤️  216 likes\n",
      "   📅 Created: 2024-09-15\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B\n",
      "   ⬇️  1,002,573 downloads, ❤️  766 likes\n",
      "   📅 Created: 2025-04-27\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-32B\n",
      "   ⬇️  926,964 downloads, ❤️  500 likes\n",
      "   📅 Created: 2025-04-27\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-0.5B\n",
      "   ⬇️  867,415 downloads, ❤️  293 likes\n",
      "   📅 Created: 2024-09-15\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-14B\n",
      "   ⬇️  706,258 downloads, ❤️  246 likes\n",
      "   📅 Created: 2025-04-27\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-0.5B-Chat\n",
      "   ⬇️  686,110 downloads, ❤️  82 likes\n",
      "   📅 Created: 2024-01-31\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-32B-AWQ\n",
      "   ⬇️  554,616 downloads, ❤️  99 likes\n",
      "   📅 Created: 2025-05-01\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-1.5B-Instruct\n",
      "   ⬇️  505,828 downloads, ❤️  148 likes\n",
      "   📅 Created: 2024-06-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "   ⬇️  446,351 downloads, ❤️  493 likes\n",
      "   📅 Created: 2025-07-28\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-1.5B\n",
      "   ⬇️  373,407 downloads, ❤️  112 likes\n",
      "   📅 Created: 2024-09-15\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Embedding-8B\n",
      "   ⬇️  364,295 downloads, ❤️  295 likes\n",
      "   📅 Created: 2025-06-03\n",
      "   🏷️  ['text-embeddings-inference', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-32B-Instruct\n",
      "   ⬇️  346,916 downloads, ❤️  290 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-14B\n",
      "   ⬇️  285,882 downloads, ❤️  124 likes\n",
      "   📅 Created: 2024-09-15\n",
      "\n",
      "📦 Qwen/Qwen2.5-3B\n",
      "   ⬇️  262,498 downloads, ❤️  140 likes\n",
      "   📅 Created: 2024-09-15\n",
      "\n",
      "📦 Qwen/Qwen2-7B-Instruct\n",
      "   ⬇️  259,716 downloads, ❤️  661 likes\n",
      "   📅 Created: 2024-06-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "   ⬇️  244,138 downloads, ❤️  527 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Reranker-0.6B\n",
      "   ⬇️  205,554 downloads, ❤️  207 likes\n",
      "   📅 Created: 2025-05-29\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "   ⬇️  202,919 downloads, ❤️  481 likes\n",
      "   📅 Created: 2025-07-31\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-1.5B\n",
      "   ⬇️  201,086 downloads, ❤️  60 likes\n",
      "   📅 Created: 2024-09-18\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-7B-Chat\n",
      "   ⬇️  190,098 downloads, ❤️  782 likes\n",
      "   📅 Created: 2023-08-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Embedding-4B\n",
      "   ⬇️  184,929 downloads, ❤️  127 likes\n",
      "   📅 Created: 2025-06-03\n",
      "   🏷️  ['text-embeddings-inference', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-0.5B\n",
      "   ⬇️  183,632 downloads, ❤️  151 likes\n",
      "   📅 Created: 2024-05-31\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-0.5B-Instruct\n",
      "   ⬇️  175,623 downloads, ❤️  192 likes\n",
      "   📅 Created: 2024-06-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\n",
      "   ⬇️  156,950 downloads, ❤️  36 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-32B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-32B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-3B-Instruct\n",
      "   ⬇️  150,862 downloads, ❤️  71 likes\n",
      "   📅 Created: 2024-11-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/QwQ-32B\n",
      "   ⬇️  150,241 downloads, ❤️  2825 likes\n",
      "   📅 Created: 2025-03-05\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-32B-Instruct-AWQ\n",
      "   ⬇️  144,906 downloads, ❤️  29 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-32B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-32B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-Instruct-2507\n",
      "   ⬇️  140,077 downloads, ❤️  223 likes\n",
      "   📅 Created: 2025-08-05\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8\n",
      "   ⬇️  138,433 downloads, ❤️  11 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-32B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-32B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B\n",
      "   ⬇️  136,203 downloads, ❤️  1027 likes\n",
      "   📅 Created: 2025-04-27\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Math-1.5B\n",
      "   ⬇️  125,286 downloads, ❤️  75 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-32B\n",
      "   ⬇️  116,150 downloads, ❤️  163 likes\n",
      "   📅 Created: 2024-09-15\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-14B-Instruct-AWQ\n",
      "   ⬇️  114,249 downloads, ❤️  12 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-14B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-14B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\n",
      "   ⬇️  112,368 downloads, ❤️  101 likes\n",
      "   📅 Created: 2025-07-22\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-Thinking-2507\n",
      "   ⬇️  106,619 downloads, ❤️  236 likes\n",
      "   📅 Created: 2025-07-29\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-72B-Instruct\n",
      "   ⬇️  101,682 downloads, ❤️  855 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-1.7B-Base\n",
      "   ⬇️  101,544 downloads, ❤️  36 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Math-7B\n",
      "   ⬇️  94,174 downloads, ❤️  98 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-1.5B\n",
      "   ⬇️  91,020 downloads, ❤️  97 likes\n",
      "   📅 Created: 2024-05-31\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-7B-Instruct-AWQ\n",
      "   ⬇️  89,205 downloads, ❤️  14 likes\n",
      "   📅 Created: 2024-09-20\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-7B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-7B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-7B\n",
      "   ⬇️  88,261 downloads, ❤️  55 likes\n",
      "   📅 Created: 2024-01-22\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\n",
      "   ⬇️  78,554 downloads, ❤️  118 likes\n",
      "   📅 Created: 2025-07-21\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['base_model:Qwen/Qwen3-235B-A22B-Instruct-2507', 'base_model:quantized:Qwen/Qwen3-235B-A22B-Instruct-2507', 'autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/QwQ-32B-Preview\n",
      "   ⬇️  76,590 downloads, ❤️  1739 likes\n",
      "   📅 Created: 2024-11-27\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-32B-Instruct', 'base_model:finetune:Qwen/Qwen2.5-32B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\n",
      "   ⬇️  74,776 downloads, ❤️  61 likes\n",
      "   📅 Created: 2025-07-28\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['base_model:Qwen/Qwen3-30B-A3B-Instruct-2507', 'base_model:quantized:Qwen/Qwen3-30B-A3B-Instruct-2507', 'autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "   ⬇️  73,622 downloads, ❤️  1919 likes\n",
      "   📅 Created: 2024-11-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-AWQ\n",
      "   ⬇️  72,177 downloads, ❤️  16 likes\n",
      "   📅 Created: 2025-05-05\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Math-7B-Instruct\n",
      "   ⬇️  71,692 downloads, ❤️  79 likes\n",
      "   📅 Created: 2024-09-19\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "   ⬇️  71,540 downloads, ❤️  641 likes\n",
      "   📅 Created: 2025-07-21\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-0.5B\n",
      "   ⬇️  70,683 downloads, ❤️  165 likes\n",
      "   📅 Created: 2024-01-22\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-VL-Chat\n",
      "   ⬇️  70,651 downloads, ❤️  369 likes\n",
      "   📅 Created: 2023-08-20\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-72B-Instruct-AWQ\n",
      "   ⬇️  69,484 downloads, ❤️  72 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-72B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-72B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Coder-480B-A35B-Instruct\n",
      "   ⬇️  59,794 downloads, ❤️  1104 likes\n",
      "   📅 Created: 2025-07-22\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-32B-Instruct-AWQ\n",
      "   ⬇️  56,703 downloads, ❤️  85 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-32B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-32B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-7B\n",
      "   ⬇️  56,024 downloads, ❤️  165 likes\n",
      "   📅 Created: 2024-06-04\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-1.8B-Chat\n",
      "   ⬇️  52,849 downloads, ❤️  59 likes\n",
      "   📅 Created: 2024-01-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-Thinking-2507-FP8\n",
      "   ⬇️  49,724 downloads, ❤️  23 likes\n",
      "   📅 Created: 2025-08-06\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Reranker-4B\n",
      "   ⬇️  49,366 downloads, ❤️  86 likes\n",
      "   📅 Created: 2025-06-03\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-8B-FP8\n",
      "   ⬇️  48,920 downloads, ❤️  39 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-14B-Base\n",
      "   ⬇️  47,081 downloads, ❤️  33 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-14B-FP8\n",
      "   ⬇️  46,575 downloads, ❤️  31 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-3B-Instruct-GGUF\n",
      "   ⬇️  44,677 downloads, ❤️  49 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-3B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-3B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-FP8\n",
      "   ⬇️  44,643 downloads, ❤️  86 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\n",
      "   ⬇️  44,582 downloads, ❤️  120 likes\n",
      "   📅 Created: 2024-09-18\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-7B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-7B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-14B\n",
      "   ⬇️  43,243 downloads, ❤️  41 likes\n",
      "   📅 Created: 2024-01-22\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-1.8B\n",
      "   ⬇️  42,748 downloads, ❤️  52 likes\n",
      "   📅 Created: 2024-01-22\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-4B-Chat\n",
      "   ⬇️  41,634 downloads, ❤️  42 likes\n",
      "   📅 Created: 2024-01-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Math-1.5B-Instruct\n",
      "   ⬇️  40,402 downloads, ❤️  42 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-Thinking-2507\n",
      "   ⬇️  40,331 downloads, ❤️  307 likes\n",
      "   📅 Created: 2025-08-05\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-72B-Instruct\n",
      "   ⬇️  39,268 downloads, ❤️  716 likes\n",
      "   📅 Created: 2024-05-28\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-1.5B-Instruct-GGUF\n",
      "   ⬇️  38,879 downloads, ❤️  54 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-1.5B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
      "   ⬇️  38,428 downloads, ❤️  78 likes\n",
      "   📅 Created: 2024-09-18\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\n",
      "   ⬇️  38,028 downloads, ❤️  55 likes\n",
      "   📅 Created: 2025-07-31\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-VL\n",
      "   ⬇️  37,543 downloads, ❤️  253 likes\n",
      "   📅 Created: 2023-08-18\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-MoE-A2.7B-Chat\n",
      "   ⬇️  34,571 downloads, ❤️  125 likes\n",
      "   📅 Created: 2024-03-14\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-MoE-A2.7B\n",
      "   ⬇️  34,535 downloads, ❤️  206 likes\n",
      "   📅 Created: 2024-02-29\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-14B-AWQ\n",
      "   ⬇️  34,459 downloads, ❤️  29 likes\n",
      "   📅 Created: 2025-05-01\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-0.5B-Instruct-GGUF\n",
      "   ⬇️  33,883 downloads, ❤️  46 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-0.5B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-8B-AWQ\n",
      "   ⬇️  33,403 downloads, ❤️  19 likes\n",
      "   📅 Created: 2025-05-03\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-7B-Chat\n",
      "   ⬇️  32,237 downloads, ❤️  177 likes\n",
      "   📅 Created: 2024-01-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-72B\n",
      "   ⬇️  28,748 downloads, ❤️  78 likes\n",
      "   📅 Created: 2024-09-15\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-GPTQ-Int4\n",
      "   ⬇️  27,219 downloads, ❤️  24 likes\n",
      "   📅 Created: 2025-05-05\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-14B-Instruct\n",
      "   ⬇️  27,198 downloads, ❤️  124 likes\n",
      "   📅 Created: 2024-11-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-8B-GGUF\n",
      "   ⬇️  27,148 downloads, ❤️  32 likes\n",
      "   📅 Created: 2025-05-03\n",
      "   🏷️  ['endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-7B\n",
      "   ⬇️  26,990 downloads, ❤️  116 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-Thinking-2507\n",
      "   ⬇️  26,654 downloads, ❤️  323 likes\n",
      "   📅 Created: 2025-07-25\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-32B-Instruct-GGUF\n",
      "   ⬇️  26,168 downloads, ❤️  172 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-32B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-32B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4\n",
      "   ⬇️  25,097 downloads, ❤️  19 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-32B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-32B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\n",
      "   ⬇️  24,702 downloads, ❤️  27 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-7B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-7B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-7B-Instruct-AWQ\n",
      "   ⬇️  24,371 downloads, ❤️  26 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-7B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-7B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-GGUF\n",
      "   ⬇️  23,780 downloads, ❤️  27 likes\n",
      "   📅 Created: 2025-05-05\n",
      "   🏷️  ['endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-4B\n",
      "   ⬇️  23,748 downloads, ❤️  35 likes\n",
      "   📅 Created: 2024-01-22\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-32B-FP8\n",
      "   ⬇️  23,581 downloads, ❤️  59 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-7B-Instruct-GGUF\n",
      "   ⬇️  23,297 downloads, ❤️  71 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-7B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-7B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-7B\n",
      "   ⬇️  22,353 downloads, ❤️  384 likes\n",
      "   📅 Created: 2023-08-03\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-3B\n",
      "   ⬇️  22,063 downloads, ❤️  36 likes\n",
      "   📅 Created: 2024-11-08\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-14B-Chat\n",
      "   ⬇️  21,514 downloads, ❤️  112 likes\n",
      "   📅 Created: 2024-01-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-Reranker-8B\n",
      "   ⬇️  21,441 downloads, ❤️  141 likes\n",
      "   📅 Created: 2025-05-29\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/QwQ-32B-GGUF\n",
      "   ⬇️  20,811 downloads, ❤️  199 likes\n",
      "   📅 Created: 2025-03-05\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-32B-GGUF\n",
      "   ⬇️  19,254 downloads, ❤️  47 likes\n",
      "   📅 Created: 2025-05-01\n",
      "   🏷️  ['endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-32B-Chat\n",
      "   ⬇️  19,120 downloads, ❤️  108 likes\n",
      "   📅 Created: 2024-04-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-FP8\n",
      "   ⬇️  18,486 downloads, ❤️  76 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
      "   ⬇️  17,810 downloads, ❤️  50 likes\n",
      "   📅 Created: 2024-11-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-Base\n",
      "   ⬇️  16,924 downloads, ❤️  47 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-0.5B\n",
      "   ⬇️  16,608 downloads, ❤️  31 likes\n",
      "   📅 Created: 2024-11-08\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-0.6B-GGUF\n",
      "   ⬇️  16,322 downloads, ❤️  28 likes\n",
      "   📅 Created: 2025-05-05\n",
      "   🏷️  ['endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-14B-GGUF\n",
      "   ⬇️  15,919 downloads, ❤️  48 likes\n",
      "   📅 Created: 2025-05-01\n",
      "   🏷️  ['endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-GGUF\n",
      "   ⬇️  15,343 downloads, ❤️  53 likes\n",
      "   📅 Created: 2025-05-05\n",
      "   🏷️  ['endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-3B-Instruct-AWQ\n",
      "   ⬇️  15,216 downloads, ❤️  4 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-3B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-3B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-Thinking-2507-FP8\n",
      "   ⬇️  13,969 downloads, ❤️  49 likes\n",
      "   📅 Created: 2025-07-25\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-1.5B-Instruct-AWQ\n",
      "   ⬇️  13,655 downloads, ❤️  8 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2-1.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-14B-Instruct-AWQ\n",
      "   ⬇️  13,428 downloads, ❤️  23 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-14B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-14B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-Thinking-2507-FP8\n",
      "   ⬇️  13,418 downloads, ❤️  30 likes\n",
      "   📅 Created: 2025-07-29\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-1.5B-Instruct-GPTQ-Int4\n",
      "   ⬇️  12,693 downloads, ❤️  4 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2-1.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-32B\n",
      "   ⬇️  11,986 downloads, ❤️  85 likes\n",
      "   📅 Created: 2024-04-01\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-14B-Instruct-1M\n",
      "   ⬇️  11,933 downloads, ❤️  317 likes\n",
      "   📅 Created: 2025-01-23\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-GPTQ-Int4\n",
      "   ⬇️  11,752 downloads, ❤️  22 likes\n",
      "   📅 Created: 2025-05-10\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-1.5B-Instruct-GGUF\n",
      "   ⬇️  11,163 downloads, ❤️  24 likes\n",
      "   📅 Created: 2024-06-07\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-57B-A14B\n",
      "   ⬇️  10,235 downloads, ❤️  52 likes\n",
      "   📅 Created: 2024-05-22\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/QwQ-32B-AWQ\n",
      "   ⬇️  10,170 downloads, ❤️  128 likes\n",
      "   📅 Created: 2025-03-05\n",
      "   🏷️  ['chat']\n",
      "\n",
      "📦 Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4\n",
      "   ⬇️  10,030 downloads, ❤️  37 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-72B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-72B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-72B-Chat\n",
      "   ⬇️  9,808 downloads, ❤️  218 likes\n",
      "   📅 Created: 2024-01-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\n",
      "   ⬇️  9,565 downloads, ❤️  21 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-14B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-14B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\n",
      "   ⬇️  9,520 downloads, ❤️  72 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-14B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-14B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-57B-A14B-Instruct\n",
      "   ⬇️  9,505 downloads, ❤️  81 likes\n",
      "   📅 Created: 2024-06-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-72B\n",
      "   ⬇️  9,414 downloads, ❤️  200 likes\n",
      "   📅 Created: 2024-05-22\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-72B\n",
      "   ⬇️  8,782 downloads, ❤️  60 likes\n",
      "   📅 Created: 2024-01-23\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-0.5B-Instruct-GGUF\n",
      "   ⬇️  7,514 downloads, ❤️  67 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['instruct', 'chat', 'base_model:Qwen/Qwen2-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2-0.5B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-Audio-Chat\n",
      "   ⬇️  6,739 downloads, ❤️  90 likes\n",
      "   📅 Created: 2023-11-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-1_8B-Chat\n",
      "   ⬇️  6,061 downloads, ❤️  121 likes\n",
      "   📅 Created: 2023-11-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8\n",
      "   ⬇️  5,809 downloads, ❤️  21 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-32B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-32B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-1.7B-FP8\n",
      "   ⬇️  5,616 downloads, ❤️  28 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-1.7B-GGUF\n",
      "   ⬇️  5,330 downloads, ❤️  15 likes\n",
      "   📅 Created: 2025-05-05\n",
      "   🏷️  ['endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-Instruct-2507-FP8\n",
      "   ⬇️  5,219 downloads, ❤️  22 likes\n",
      "   📅 Created: 2025-08-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['base_model:Qwen/Qwen3-4B-Instruct-2507', 'base_model:quantized:Qwen/Qwen3-4B-Instruct-2507', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-3B-Instruct-GGUF\n",
      "   ⬇️  5,080 downloads, ❤️  41 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-3B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-3B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-3B-Instruct-AWQ\n",
      "   ⬇️  5,046 downloads, ❤️  10 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-3B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-3B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-1_8B\n",
      "   ⬇️  4,962 downloads, ❤️  70 likes\n",
      "   📅 Created: 2023-11-30\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-FP8\n",
      "   ⬇️  4,958 downloads, ❤️  27 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-0.6B-FP8\n",
      "   ⬇️  4,428 downloads, ❤️  47 likes\n",
      "   📅 Created: 2025-04-28\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-14B\n",
      "   ⬇️  4,351 downloads, ❤️  211 likes\n",
      "   📅 Created: 2023-09-24\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-7B-Chat-GGUF\n",
      "   ⬇️  4,308 downloads, ❤️  68 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-0.5B-Chat-GGUF\n",
      "   ⬇️  4,233 downloads, ❤️  32 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\n",
      "   ⬇️  4,212 downloads, ❤️  15 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-7B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-7B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF\n",
      "   ⬇️  4,161 downloads, ❤️  8 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-0.5B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-32B\n",
      "   ⬇️  3,982 downloads, ❤️  134 likes\n",
      "   📅 Created: 2024-11-08\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-0.6B-GPTQ-Int8\n",
      "   ⬇️  3,919 downloads, ❤️  8 likes\n",
      "   📅 Created: 2025-05-08\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-7B-Instruct-GGUF\n",
      "   ⬇️  3,861 downloads, ❤️  175 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-7B-Instruct', 'base_model:quantized:Qwen/Qwen2-7B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/CodeQwen1.5-7B-Chat\n",
      "   ⬇️  3,804 downloads, ❤️  339 likes\n",
      "   📅 Created: 2024-04-15\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-72B-Instruct-GPTQ-Int4\n",
      "   ⬇️  3,786 downloads, ❤️  32 likes\n",
      "   📅 Created: 2024-06-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-72B-Instruct', 'base_model:quantized:Qwen/Qwen2-72B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8\n",
      "   ⬇️  3,718 downloads, ❤️  21 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-14B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-14B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-14B\n",
      "   ⬇️  3,608 downloads, ❤️  44 likes\n",
      "   📅 Created: 2024-11-08\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-72B\n",
      "   ⬇️  3,383 downloads, ❤️  357 likes\n",
      "   📅 Created: 2023-11-26\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-110B-Chat-GPTQ-Int4\n",
      "   ⬇️  3,312 downloads, ❤️  17 likes\n",
      "   📅 Created: 2024-04-26\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-110B-Chat\n",
      "   ⬇️  3,173 downloads, ❤️  127 likes\n",
      "   📅 Created: 2024-04-25\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-Audio\n",
      "   ⬇️  3,114 downloads, ❤️  134 likes\n",
      "   📅 Created: 2023-11-30\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
      "   ⬇️  2,845 downloads, ❤️  8 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-0.5B-Instruct']\n",
      "\n",
      "📦 Qwen/Qwen2.5-14B-Instruct-GGUF\n",
      "   ⬇️  2,832 downloads, ❤️  37 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-14B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-14B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Math-72B-Instruct\n",
      "   ⬇️  2,814 downloads, ❤️  30 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-VL-Chat-Int4\n",
      "   ⬇️  2,651 downloads, ❤️  92 likes\n",
      "   📅 Created: 2023-08-31\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-1.8B-Chat-GGUF\n",
      "   ⬇️  2,621 downloads, ❤️  18 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF\n",
      "   ⬇️  2,582 downloads, ❤️  24 likes\n",
      "   📅 Created: 2024-09-18\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-1.5B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-72B-Chat-GPTQ-Int4\n",
      "   ⬇️  2,464 downloads, ❤️  37 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4\n",
      "   ⬇️  2,405 downloads, ❤️  5 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-14B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-14B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\n",
      "   ⬇️  2,282 downloads, ❤️  1 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-1.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/CodeQwen1.5-7B\n",
      "   ⬇️  2,229 downloads, ❤️  99 likes\n",
      "   📅 Created: 2024-04-15\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-32B-Chat-GPTQ-Int4\n",
      "   ⬇️  2,182 downloads, ❤️  30 likes\n",
      "   📅 Created: 2024-04-01\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-72B-Instruct-AWQ\n",
      "   ⬇️  2,182 downloads, ❤️  40 likes\n",
      "   📅 Created: 2024-06-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-72B-Instruct', 'base_model:quantized:Qwen/Qwen2-72B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-8B-MLX-bf16\n",
      "   ⬇️  2,130 downloads, ❤️  4 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen2-7B-Instruct-AWQ\n",
      "   ⬇️  2,041 downloads, ❤️  21 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-7B-Instruct', 'base_model:quantized:Qwen/Qwen2-7B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\n",
      "   ⬇️  1,998 downloads, ❤️  3 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-3B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-3B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-14B-Chat\n",
      "   ⬇️  1,987 downloads, ❤️  365 likes\n",
      "   📅 Created: 2023-09-24\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-7B-Instruct-GPTQ-Int4\n",
      "   ⬇️  1,736 downloads, ❤️  28 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-7B-Instruct', 'base_model:quantized:Qwen/Qwen2-7B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-1.5B-Instruct-AWQ\n",
      "   ⬇️  1,719 downloads, ❤️  6 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-1.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-0.6B-MLX-4bit\n",
      "   ⬇️  1,665 downloads, ❤️  8 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen-7B-Chat-Int4\n",
      "   ⬇️  1,658 downloads, ❤️  75 likes\n",
      "   📅 Created: 2023-08-20\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-0.5B-Instruct-AWQ\n",
      "   ⬇️  1,620 downloads, ❤️  10 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-0.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4\n",
      "   ⬇️  1,606 downloads, ❤️  8 likes\n",
      "   📅 Created: 2024-09-20\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-7B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-7B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-GGUF\n",
      "   ⬇️  1,603 downloads, ❤️  9 likes\n",
      "   📅 Created: 2025-05-11\n",
      "   🏷️  ['endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-1.7B-GPTQ-Int8\n",
      "   ⬇️  1,577 downloads, ❤️  3 likes\n",
      "   📅 Created: 2025-05-08\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-Math-1.5B\n",
      "   ⬇️  1,547 downloads, ❤️  14 likes\n",
      "   📅 Created: 2024-08-08\n",
      "   🏷️  ['chat']\n",
      "\n",
      "📦 Qwen/Qwen2-Math-7B-Instruct\n",
      "   ⬇️  1,517 downloads, ❤️  42 likes\n",
      "   📅 Created: 2024-08-08\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ\n",
      "   ⬇️  1,500 downloads, ❤️  3 likes\n",
      "   📅 Created: 2024-09-20\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-1.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/CodeQwen1.5-7B-Chat-GGUF\n",
      "   ⬇️  1,484 downloads, ❤️  109 likes\n",
      "   📅 Created: 2024-04-15\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-4B-Chat-AWQ\n",
      "   ⬇️  1,432 downloads, ❤️  3 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8\n",
      "   ⬇️  1,373 downloads, ❤️  27 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-72B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-72B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Math-72B\n",
      "   ⬇️  1,370 downloads, ❤️  15 likes\n",
      "   📅 Created: 2024-09-16\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-1_8B-Chat-Int4\n",
      "   ⬇️  1,326 downloads, ❤️  36 likes\n",
      "   📅 Created: 2023-11-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-32B-Instruct-GGUF\n",
      "   ⬇️  1,325 downloads, ❤️  38 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-32B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-32B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-0.6B-MLX-bf16\n",
      "   ⬇️  1,273 downloads, ❤️  5 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen2-Math-7B\n",
      "   ⬇️  1,261 downloads, ❤️  14 likes\n",
      "   📅 Created: 2024-08-08\n",
      "   🏷️  ['chat']\n",
      "\n",
      "📦 Qwen/Qwen-72B-Chat\n",
      "   ⬇️  1,256 downloads, ❤️  157 likes\n",
      "   📅 Created: 2023-11-29\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-14B-Chat-GGUF\n",
      "   ⬇️  1,239 downloads, ❤️  66 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-Math-1.5B-Instruct\n",
      "   ⬇️  1,228 downloads, ❤️  21 likes\n",
      "   📅 Created: 2024-08-08\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-MLX-4bit\n",
      "   ⬇️  1,203 downloads, ❤️  14 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\n",
      "   ⬇️  1,190 downloads, ❤️  21 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-7B-Chat-AWQ\n",
      "   ⬇️  1,151 downloads, ❤️  13 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-110B\n",
      "   ⬇️  1,126 downloads, ❤️  99 likes\n",
      "   📅 Created: 2024-04-25\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-Math-72B-Instruct\n",
      "   ⬇️  1,087 downloads, ❤️  88 likes\n",
      "   📅 Created: 2024-08-08\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8\n",
      "   ⬇️  1,082 downloads, ❤️  3 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-3B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-3B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-7B-Chat-GPTQ-Int4\n",
      "   ⬇️  1,073 downloads, ❤️  18 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-0.5B-Instruct-MLX\n",
      "   ⬇️  1,073 downloads, ❤️  10 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8\n",
      "   ⬇️  1,048 downloads, ❤️  5 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-14B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-14B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int8\n",
      "   ⬇️  1,046 downloads, ❤️  9 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-0.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8\n",
      "   ⬇️  1,023 downloads, ❤️  4 likes\n",
      "   📅 Created: 2024-09-20\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-7B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-7B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4\n",
      "   ⬇️  1,015 downloads, ❤️  1 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-3B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-3B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-72B-Chat-AWQ\n",
      "   ⬇️  998 downloads, ❤️  24 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-14B-Chat-AWQ\n",
      "   ⬇️  965 downloads, ❤️  23 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-7B-Chat-Int8\n",
      "   ⬇️  957 downloads, ❤️  8 likes\n",
      "   📅 Created: 2023-10-11\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-4B-Chat-GGUF\n",
      "   ⬇️  943 downloads, ❤️  13 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ\n",
      "   ⬇️  938 downloads, ❤️  3 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-0.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-72B-Instruct-GGUF\n",
      "   ⬇️  932 downloads, ❤️  43 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-72B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-72B-Instruct', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-7B-Instruct-GPTQ-Int8\n",
      "   ⬇️  917 downloads, ❤️  17 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-7B-Instruct', 'base_model:quantized:Qwen/Qwen2-7B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-14B-Chat-Int4\n",
      "   ⬇️  912 downloads, ❤️  101 likes\n",
      "   📅 Created: 2023-09-24\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8\n",
      "   ⬇️  910 downloads, ❤️  1 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-0.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4\n",
      "   ⬇️  899 downloads, ❤️  46 likes\n",
      "   📅 Created: 2024-03-20\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int4\n",
      "   ⬇️  896 downloads, ❤️  7 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-0.5B-Instruct-AWQ\n",
      "   ⬇️  893 downloads, ❤️  4 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2-0.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\n",
      "   ⬇️  888 downloads, ❤️  26 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-0.5B-Instruct-GPTQ-Int8\n",
      "   ⬇️  882 downloads, ❤️  4 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2-0.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-7B-Instruct-MLX\n",
      "   ⬇️  880 downloads, ❤️  5 likes\n",
      "   📅 Created: 2024-09-18\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8\n",
      "   ⬇️  878 downloads, ❤️  3 likes\n",
      "   📅 Created: 2024-09-20\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-1.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int4\n",
      "   ⬇️  873 downloads, ❤️  13 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int8\n",
      "   ⬇️  869 downloads, ❤️  3 likes\n",
      "   📅 Created: 2024-09-17\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-1.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-1.7B-MLX-6bit\n",
      "   ⬇️  860 downloads, ❤️  2 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen2-1.5B-Instruct-GPTQ-Int8\n",
      "   ⬇️  846 downloads, ❤️  4 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2-1.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4\n",
      "   ⬇️  845 downloads, ❤️  2 likes\n",
      "   📅 Created: 2024-09-20\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-1.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-1.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int8\n",
      "   ⬇️  809 downloads, ❤️  4 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-32B-MLX-bf16\n",
      "   ⬇️  502 downloads, ❤️  6 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen2-72B-Instruct-GGUF\n",
      "   ⬇️  486 downloads, ❤️  31 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['instruct', 'chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-32B-Chat-GGUF\n",
      "   ⬇️  418 downloads, ❤️  53 likes\n",
      "   📅 Created: 2024-04-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-1.7B-MLX-bf16\n",
      "   ⬇️  371 downloads, ❤️  4 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-MLX-4bit\n",
      "   ⬇️  348 downloads, ❤️  10 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen2-57B-A14B-Instruct-GGUF\n",
      "   ⬇️  342 downloads, ❤️  15 likes\n",
      "   📅 Created: 2024-06-15\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['instruct', 'chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-MLX-4bit\n",
      "   ⬇️  317 downloads, ❤️  4 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-32B-MLX-8bit\n",
      "   ⬇️  279 downloads, ❤️  9 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen2-Math-72B\n",
      "   ⬇️  273 downloads, ❤️  30 likes\n",
      "   📅 Created: 2024-08-08\n",
      "   🏷️  ['chat']\n",
      "\n",
      "📦 Qwen/Qwen3-8B-MLX-4bit\n",
      "   ⬇️  269 downloads, ❤️  4 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-1.7B-MLX-4bit\n",
      "   ⬇️  260 downloads, ❤️  3 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-32B-MLX-4bit\n",
      "   ⬇️  244 downloads, ❤️  6 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen2-0.5B-Instruct-GPTQ-Int4\n",
      "   ⬇️  237 downloads, ❤️  14 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2-0.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-MLX-8bit\n",
      "   ⬇️  235 downloads, ❤️  8 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen3-0.6B-MLX-8bit\n",
      "   ⬇️  222 downloads, ❤️  2 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-8B-MLX-8bit\n",
      "   ⬇️  219 downloads, ❤️  4 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-MLX-bf16\n",
      "   ⬇️  219 downloads, ❤️  7 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen3-14B-MLX-4bit\n",
      "   ⬇️  199 downloads, ❤️  5 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-4B-MLX-6bit\n",
      "   ⬇️  192 downloads, ❤️  3 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-0.6B-MLX-6bit\n",
      "   ⬇️  160 downloads, ❤️  3 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-4B-MLX-bf16\n",
      "   ⬇️  152 downloads, ❤️  3 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-MLX-bf16\n",
      "   ⬇️  150 downloads, ❤️  5 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-MLX-8bit\n",
      "   ⬇️  136 downloads, ❤️  7 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen1.5-0.5B-Chat-AWQ\n",
      "   ⬇️  133 downloads, ❤️  7 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-32B-Chat-AWQ\n",
      "   ⬇️  125 downloads, ❤️  18 likes\n",
      "   📅 Created: 2024-04-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-4B-Chat-GPTQ-Int8\n",
      "   ⬇️  117 downloads, ❤️  6 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-14B-MLX-bf16\n",
      "   ⬇️  115 downloads, ❤️  3 likes\n",
      "   📅 Created: 2025-06-12\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8\n",
      "   ⬇️  113 downloads, ❤️  1 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-3B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-3B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-MLX-8bit\n",
      "   ⬇️  109 downloads, ❤️  2 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen2-72B-Instruct-GPTQ-Int8\n",
      "   ⬇️  105 downloads, ❤️  15 likes\n",
      "   📅 Created: 2024-06-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-72B-Instruct', 'base_model:quantized:Qwen/Qwen2-72B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-14B-MLX-8bit\n",
      "   ⬇️  103 downloads, ❤️  3 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen3-8B-MLX-6bit\n",
      "   ⬇️  101 downloads, ❤️  5 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/CodeQwen1.5-7B-Chat-AWQ\n",
      "   ⬇️  98 downloads, ❤️  14 likes\n",
      "   📅 Created: 2024-04-15\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-4B-Chat-GPTQ-Int4\n",
      "   ⬇️  94 downloads, ❤️  6 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-32B-MLX-6bit\n",
      "   ⬇️  88 downloads, ❤️  2 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen3-30B-A3B-MLX-6bit\n",
      "   ⬇️  84 downloads, ❤️  3 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen3-14B-MLX-6bit\n",
      "   ⬇️  83 downloads, ❤️  2 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4\n",
      "   ⬇️  78 downloads, ❤️  1 likes\n",
      "   📅 Created: 2024-11-09\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2.5-Coder-0.5B-Instruct', 'base_model:quantized:Qwen/Qwen2.5-Coder-0.5B-Instruct', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-72B-Chat-GGUF\n",
      "   ⬇️  72 downloads, ❤️  64 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-235B-A22B-MLX-6bit\n",
      "   ⬇️  70 downloads, ❤️  3 likes\n",
      "   📅 Created: 2025-06-11\n",
      "\n",
      "📦 Qwen/Qwen3-1.7B-MLX-8bit\n",
      "   ⬇️  69 downloads, ❤️  2 likes\n",
      "   📅 Created: 2025-05-23\n",
      "\n",
      "📦 Qwen/Qwen-72B-Chat-Int4\n",
      "   ⬇️  51 downloads, ❤️  46 likes\n",
      "   📅 Created: 2023-11-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-14B-Chat-Int8\n",
      "   ⬇️  43 downloads, ❤️  6 likes\n",
      "   📅 Created: 2023-10-12\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-1.8B-Chat-AWQ\n",
      "   ⬇️  43 downloads, ❤️  4 likes\n",
      "   📅 Created: 2024-02-03\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-110B-Chat-GGUF\n",
      "   ⬇️  34 downloads, ❤️  14 likes\n",
      "   📅 Created: 2024-04-28\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-1_8B-Chat-Int8\n",
      "   ⬇️  33 downloads, ❤️  5 likes\n",
      "   📅 Created: 2023-11-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int8\n",
      "   ⬇️  29 downloads, ❤️  2 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen-72B-Chat-Int8\n",
      "   ⬇️  23 downloads, ❤️  17 likes\n",
      "   📅 Created: 2023-11-30\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['autotrain_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-72B-Chat-GPTQ-Int8\n",
      "   ⬇️  22 downloads, ❤️  7 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4\n",
      "   ⬇️  19 downloads, ❤️  23 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'base_model:Qwen/Qwen2-57B-A14B-Instruct', 'base_model:quantized:Qwen/Qwen2-57B-A14B-Instruct', 'autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2-1.5B-Instruct-MLX\n",
      "   ⬇️  19 downloads, ❤️  4 likes\n",
      "   📅 Created: 2024-06-06\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-110B-Chat-AWQ\n",
      "   ⬇️  17 downloads, ❤️  9 likes\n",
      "   📅 Created: 2024-04-27\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen1.5-14B-Chat-GPTQ-Int8\n",
      "   ⬇️  13 downloads, ❤️  11 likes\n",
      "   📅 Created: 2024-02-04\n",
      "   🤖 Chat/Instruct model ✓\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/CodeQwen1.5-7B-AWQ\n",
      "   ⬇️  10 downloads, ❤️  2 likes\n",
      "   📅 Created: 2024-04-21\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Get Qwen text-generation models specifically\n",
    "qwen_text_models = api.list_models(\n",
    "    author=\"Qwen\",\n",
    "    task=\"text-generation\",\n",
    "    limit=None,\n",
    "    sort=\"downloads\",\n",
    "    direction=-1\n",
    ")\n",
    "\n",
    "qwen_text_list = list(qwen_text_models)\n",
    "print(f\"Found {len(qwen_text_list)} Qwen text-generation models:\")\n",
    "\n",
    "# Show with more details\n",
    "for model in qwen_text_list:\n",
    "    print(f\"\\n📦 {model.id}\")\n",
    "    print(f\"   ⬇️  {model.downloads:,} downloads, ❤️  {model.likes} likes\")\n",
    "    print(f\"   📅 Created: {model.created_at.strftime('%Y-%m-%d') if model.created_at else 'N/A'}\")\n",
    "    \n",
    "    # Look for chat/instruct models\n",
    "    is_instruct = any(keyword in model.id.lower() for keyword in ['instruct', 'chat'])\n",
    "    if is_instruct:\n",
    "        print(f\"   🤖 Chat/Instruct model ✓\")\n",
    "    \n",
    "    # Show relevant tags\n",
    "    if model.tags:\n",
    "        relevant_tags = [tag for tag in model.tags if \n",
    "                        any(keyword in tag.lower() for keyword in \n",
    "                            ['instruct', 'chat', 'inference', 'compatible'])]\n",
    "        if relevant_tags:\n",
    "            print(f\"   🏷️  {relevant_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc108f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Qwen models for testing (151 found):\n",
      "  - Qwen/Qwen2.5-14B-Instruct\n",
      "  - Qwen/Qwen2.5-7B-Instruct\n",
      "  - Qwen/Qwen2.5-1.5B-Instruct\n",
      "  - Qwen/Qwen2.5-7B-Instruct-1M\n",
      "  - Qwen/Qwen2.5-3B-Instruct\n",
      "  - Qwen/Qwen2.5-0.5B-Instruct\n",
      "  - Qwen/Qwen1.5-0.5B-Chat\n",
      "  - Qwen/Qwen2-1.5B-Instruct\n",
      "\n",
      "Testing top 5 Qwen models...\n",
      "Testing 5 HuggingFace model(s)...\n",
      "==================================================\n",
      "Testing: Qwen/Qwen2.5-14B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-14B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-1.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct-1M\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-7B-Instruct-1M' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-3B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-3B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "==================================================\n",
      "Summary:\n",
      "  Total tested: 5\n",
      "  Supported: 1\n",
      "  Correct answers: 1\n",
      "\n",
      "✅ Working models:\n",
      "  - Qwen/Qwen2.5-7B-Instruct\n",
      "Working Qwen models: ['Qwen/Qwen2.5-7B-Instruct']\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Get Qwen text-generation models\n",
    "qwen_models = api.list_models(\n",
    "    author=\"Qwen\", \n",
    "    task=\"text-generation\",\n",
    "    limit=None,\n",
    "    sort=\"downloads\", \n",
    "    direction=-1\n",
    ")\n",
    "\n",
    "# Filter for likely chat/instruct models\n",
    "good_qwen_models = []\n",
    "\n",
    "for model in qwen_models:\n",
    "    # Look for instruct/chat indicators\n",
    "    is_instruct = any(keyword in model.id.lower() for keyword in [\n",
    "        'instruct', 'chat', 'conversation'\n",
    "    ])\n",
    "    \n",
    "    # Check if it has good download numbers and is public\n",
    "    if is_instruct and not model.private and model.downloads > 100:\n",
    "        good_qwen_models.append(model.id)\n",
    "\n",
    "print(f\"Best Qwen models for testing ({len(good_qwen_models)} found):\")\n",
    "for model_id in good_qwen_models[:8]:  # Top 8\n",
    "    print(f\"  - {model_id}\")\n",
    "\n",
    "# Test these models\n",
    "if good_qwen_models:\n",
    "    print(f\"\\nTesting top {min(5, len(good_qwen_models))} Qwen models...\")\n",
    "    result = check_model_supported(good_qwen_models[:5])\n",
    "    print(f\"Working Qwen models: {result['correct_models']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb903afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Qwen text-generation models...\n",
      "\n",
      "🎯 Best Qwen instruct/chat models (sorted by release date, newest first):\n",
      "================================================================================\n",
      " 1. 📦 Qwen/Qwen3-4B-Instruct-2507-FP8\n",
      "    📅 Released: 2025-08-06\n",
      "    ⬇️  Downloads: 5,219 | ❤️  Likes: 22\n",
      "    📏 Size: Unknown size\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      " 2. 📦 Qwen/Qwen3-4B-Instruct-2507\n",
      "    📅 Released: 2025-08-05\n",
      "    ⬇️  Downloads: 140,077 | ❤️  Likes: 223\n",
      "    📏 Size: Unknown size\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      " 3. 📦 Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\n",
      "    📅 Released: 2025-07-31\n",
      "    ⬇️  Downloads: 38,028 | ❤️  Likes: 55\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: autotrain_compatible, endpoints_compatible\n",
      "\n",
      " 4. 📦 Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "    📅 Released: 2025-07-31\n",
      "    ⬇️  Downloads: 202,919 | ❤️  Likes: 481\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: autotrain_compatible, endpoints_compatible\n",
      "\n",
      " 5. 📦 Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\n",
      "    📅 Released: 2025-07-28\n",
      "    ⬇️  Downloads: 74,776 | ❤️  Likes: 61\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: autotrain_compatible, endpoints_compatible\n",
      "\n",
      " 6. 📦 Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "    📅 Released: 2025-07-28\n",
      "    ⬇️  Downloads: 446,351 | ❤️  Likes: 493\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: autotrain_compatible, endpoints_compatible\n",
      "\n",
      " 7. 📦 Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\n",
      "    📅 Released: 2025-07-22\n",
      "    ⬇️  Downloads: 112,368 | ❤️  Likes: 101\n",
      "    📏 Size: Unknown size\n",
      "    🤖 Inference: autotrain_compatible, endpoints_compatible\n",
      "\n",
      " 8. 📦 Qwen/Qwen3-Coder-480B-A35B-Instruct\n",
      "    📅 Released: 2025-07-22\n",
      "    ⬇️  Downloads: 59,794 | ❤️  Likes: 1104\n",
      "    📏 Size: Unknown size\n",
      "    🤖 Inference: autotrain_compatible, endpoints_compatible\n",
      "\n",
      " 9. 📦 Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\n",
      "    📅 Released: 2025-07-21\n",
      "    ⬇️  Downloads: 78,554 | ❤️  Likes: 118\n",
      "    📏 Size: Unknown size\n",
      "    🤖 Inference: autotrain_compatible, endpoints_compatible\n",
      "\n",
      "10. 📦 Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "    📅 Released: 2025-07-21\n",
      "    ⬇️  Downloads: 71,540 | ❤️  Likes: 641\n",
      "    📏 Size: Unknown size\n",
      "    🤖 Inference: autotrain_compatible, endpoints_compatible\n",
      "\n",
      "11. 📦 Qwen/Qwen2.5-7B-Instruct-1M\n",
      "    📅 Released: 2025-01-23\n",
      "    ⬇️  Downloads: 1,419,379 | ❤️  Likes: 346\n",
      "    📏 Size: 7B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "12. 📦 Qwen/Qwen2.5-14B-Instruct-1M\n",
      "    📅 Released: 2025-01-23\n",
      "    ⬇️  Downloads: 11,933 | ❤️  Likes: 317\n",
      "    📏 Size: 14B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "13. 📦 Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 4,161 | ❤️  Likes: 8\n",
      "    📏 Size: 0.5B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "14. 📦 Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 938 | ❤️  Likes: 3\n",
      "    📏 Size: 0.5B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "15. 📦 Qwen/Qwen2.5-Coder-3B-Instruct-GGUF\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 5,080 | ❤️  Likes: 41\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "16. 📦 Qwen/Qwen2.5-Coder-3B-Instruct-AWQ\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 15,216 | ❤️  Likes: 4\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "17. 📦 Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 9,520 | ❤️  Likes: 72\n",
      "    📏 Size: 14B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "18. 📦 Qwen/Qwen2.5-Coder-14B-Instruct-AWQ\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 114,249 | ❤️  Likes: 12\n",
      "    📏 Size: 14B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "19. 📦 Qwen/Qwen2.5-Coder-32B-Instruct-GGUF\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 26,168 | ❤️  Likes: 172\n",
      "    📏 Size: 32B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "20. 📦 Qwen/Qwen2.5-Coder-32B-Instruct-AWQ\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 144,906 | ❤️  Likes: 29\n",
      "    📏 Size: 32B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "21. 📦 Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 25,097 | ❤️  Likes: 19\n",
      "    📏 Size: 32B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "22. 📦 Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 5,809 | ❤️  Likes: 21\n",
      "    📏 Size: 32B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "23. 📦 Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 2,405 | ❤️  Likes: 5\n",
      "    📏 Size: 14B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "24. 📦 Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 1,048 | ❤️  Likes: 5\n",
      "    📏 Size: 14B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "25. 📦 Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 1,015 | ❤️  Likes: 1\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "26. 📦 Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 113 | ❤️  Likes: 1\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "27. 📦 Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 78 | ❤️  Likes: 1\n",
      "    📏 Size: 0.5B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "28. 📦 Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8\n",
      "    📅 Released: 2024-11-09\n",
      "    ⬇️  Downloads: 910 | ❤️  Likes: 1\n",
      "    📏 Size: 0.5B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "29. 📦 Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "    📅 Released: 2024-11-06\n",
      "    ⬇️  Downloads: 73,622 | ❤️  Likes: 1919\n",
      "    📏 Size: 32B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "30. 📦 Qwen/Qwen2.5-Coder-14B-Instruct\n",
      "    📅 Released: 2024-11-06\n",
      "    ⬇️  Downloads: 27,198 | ❤️  Likes: 124\n",
      "    📏 Size: 14B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "31. 📦 Qwen/Qwen2.5-Coder-3B-Instruct\n",
      "    📅 Released: 2024-11-06\n",
      "    ⬇️  Downloads: 150,862 | ❤️  Likes: 71\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "32. 📦 Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
      "    📅 Released: 2024-11-06\n",
      "    ⬇️  Downloads: 17,810 | ❤️  Likes: 50\n",
      "    📏 Size: 0.5B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "33. 📦 Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ\n",
      "    📅 Released: 2024-09-20\n",
      "    ⬇️  Downloads: 1,500 | ❤️  Likes: 3\n",
      "    📏 Size: 1.5B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "34. 📦 Qwen/Qwen2.5-Coder-7B-Instruct-AWQ\n",
      "    📅 Released: 2024-09-20\n",
      "    ⬇️  Downloads: 89,205 | ❤️  Likes: 14\n",
      "    📏 Size: 7B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "35. 📦 Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8\n",
      "    📅 Released: 2024-09-20\n",
      "    ⬇️  Downloads: 1,023 | ❤️  Likes: 4\n",
      "    📏 Size: 7B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "36. 📦 Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4\n",
      "    📅 Released: 2024-09-20\n",
      "    ⬇️  Downloads: 1,606 | ❤️  Likes: 8\n",
      "    📏 Size: 7B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "37. 📦 Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8\n",
      "    📅 Released: 2024-09-20\n",
      "    ⬇️  Downloads: 878 | ❤️  Likes: 3\n",
      "    📏 Size: 1.5B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "38. 📦 Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4\n",
      "    📅 Released: 2024-09-20\n",
      "    ⬇️  Downloads: 845 | ❤️  Likes: 2\n",
      "    📏 Size: 1.5B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "39. 📦 Qwen/Qwen2.5-Math-7B-Instruct\n",
      "    📅 Released: 2024-09-19\n",
      "    ⬇️  Downloads: 71,692 | ❤️  Likes: 79\n",
      "    📏 Size: 7B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "40. 📦 Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF\n",
      "    📅 Released: 2024-09-18\n",
      "    ⬇️  Downloads: 2,582 | ❤️  Likes: 24\n",
      "    📏 Size: 1.5B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "41. 📦 Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\n",
      "    📅 Released: 2024-09-18\n",
      "    ⬇️  Downloads: 44,582 | ❤️  Likes: 120\n",
      "    📏 Size: 7B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "42. 📦 Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
      "    📅 Released: 2024-09-18\n",
      "    ⬇️  Downloads: 38,428 | ❤️  Likes: 78\n",
      "    📏 Size: 1.5B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "43. 📦 Qwen/Qwen2-7B-Instruct-MLX\n",
      "    📅 Released: 2024-09-18\n",
      "    ⬇️  Downloads: 880 | ❤️  Likes: 5\n",
      "    📏 Size: 7B\n",
      "\n",
      "44. 📦 Qwen/Qwen2.5-1.5B-Instruct\n",
      "    📅 Released: 2024-09-17\n",
      "    ⬇️  Downloads: 2,998,900 | ❤️  Likes: 490\n",
      "    📏 Size: 1.5B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "45. 📦 Qwen/Qwen2.5-3B-Instruct\n",
      "    📅 Released: 2024-09-17\n",
      "    ⬇️  Downloads: 1,212,575 | ❤️  Likes: 290\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: autotrain_compatible, text-generation-inference, endpoints_compatible\n",
      "\n",
      "46. 📦 Qwen/Qwen2.5-72B-Instruct-GGUF\n",
      "    📅 Released: 2024-09-17\n",
      "    ⬇️  Downloads: 932 | ❤️  Likes: 43\n",
      "    📏 Size: 72B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "47. 📦 Qwen/Qwen2.5-32B-Instruct-GGUF\n",
      "    📅 Released: 2024-09-17\n",
      "    ⬇️  Downloads: 1,325 | ❤️  Likes: 38\n",
      "    📏 Size: 32B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "48. 📦 Qwen/Qwen2.5-14B-Instruct-GGUF\n",
      "    📅 Released: 2024-09-17\n",
      "    ⬇️  Downloads: 2,832 | ❤️  Likes: 37\n",
      "    📏 Size: 14B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "49. 📦 Qwen/Qwen2.5-7B-Instruct-GGUF\n",
      "    📅 Released: 2024-09-17\n",
      "    ⬇️  Downloads: 23,297 | ❤️  Likes: 71\n",
      "    📏 Size: 7B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "50. 📦 Qwen/Qwen2.5-3B-Instruct-GGUF\n",
      "    📅 Released: 2024-09-17\n",
      "    ⬇️  Downloads: 44,677 | ❤️  Likes: 49\n",
      "    📏 Size: 3B\n",
      "    🤖 Inference: endpoints_compatible\n",
      "\n",
      "🧪 Testing top 10 newest Qwen models...\n",
      "Models to test: ['Qwen/Qwen3-4B-Instruct-2507-FP8', 'Qwen/Qwen3-4B-Instruct-2507', 'Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8', 'Qwen/Qwen3-Coder-30B-A3B-Instruct', 'Qwen/Qwen3-30B-A3B-Instruct-2507-FP8', 'Qwen/Qwen3-30B-A3B-Instruct-2507', 'Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8', 'Qwen/Qwen3-Coder-480B-A35B-Instruct', 'Qwen/Qwen3-235B-A22B-Instruct-2507-FP8', 'Qwen/Qwen3-235B-A22B-Instruct-2507']\n",
      "Testing 10 HuggingFace model(s)...\n",
      "==================================================\n",
      "Testing: Qwen/Qwen3-4B-Instruct-2507-FP8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-4B-Instruct-2507-FP8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-4B-Instruct-2507\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-30B-A3B-Instruct-2507-FP8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-30B-A3B-Instruct-2507' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-Coder-480B-A35B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-235B-A22B-Instruct-2507-FP8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "==================================================\n",
      "Summary:\n",
      "  Total tested: 10\n",
      "  Supported: 5\n",
      "  Correct answers: 5\n",
      "\n",
      "✅ Working models:\n",
      "  - Qwen/Qwen3-4B-Instruct-2507\n",
      "  - Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "  - Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\n",
      "  - Qwen/Qwen3-Coder-480B-A35B-Instruct\n",
      "  - Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "\n",
      "✅ Working Qwen models (5 out of 10):\n",
      "  - Qwen/Qwen3-4B-Instruct-2507 (Released: 2025-08-05)\n",
      "  - Qwen/Qwen3-Coder-30B-A3B-Instruct (Released: 2025-07-31)\n",
      "  - Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8 (Released: 2025-07-22)\n",
      "  - Qwen/Qwen3-Coder-480B-A35B-Instruct (Released: 2025-07-22)\n",
      "  - Qwen/Qwen3-235B-A22B-Instruct-2507 (Released: 2025-07-21)\n",
      "\n",
      "📊 Release timeline summary:\n",
      "  2025-08: 2 models\n",
      "  2025-07: 8 models\n",
      "  2025-01: 2 models\n",
      "  2024-11: 20 models\n",
      "  2024-09: 49 models\n",
      "  2024-08: 3 models\n",
      "  2024-06: 22 models\n",
      "  2024-05: 1 models\n",
      "  2024-04: 9 models\n",
      "  2024-03: 2 models\n",
      "  2024-02: 20 models\n",
      "  2024-01: 6 models\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from datetime import datetime\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Get Qwen text-generation models (get more to have a good selection)\n",
    "print(\"Fetching Qwen text-generation models...\")\n",
    "qwen_models = api.list_models(\n",
    "    author=\"Qwen\", \n",
    "    task=\"text-generation\",\n",
    "    limit=None,  # Get all\n",
    "    sort=\"lastModified\",  # Sort by last modified instead of downloads\n",
    "    direction=-1\n",
    ")\n",
    "\n",
    "# Filter for likely chat/instruct models with release dates\n",
    "good_qwen_models = []\n",
    "\n",
    "for model in qwen_models:\n",
    "    # Look for instruct/chat indicators\n",
    "    is_instruct = any(keyword in model.id.lower() for keyword in [\n",
    "        'instruct', 'chat', 'conversation'\n",
    "    ])\n",
    "    \n",
    "    # Check if it has good stats and is public\n",
    "    if is_instruct and not model.private and model.downloads > 50:  # Lower threshold for newer models\n",
    "        good_qwen_models.append({\n",
    "            'id': model.id,\n",
    "            'created_at': model.created_at,\n",
    "            'downloads': model.downloads,\n",
    "            'likes': model.likes,\n",
    "            'tags': model.tags\n",
    "        })\n",
    "\n",
    "# Sort by creation date (newest first)\n",
    "good_qwen_models.sort(key=lambda x: x['created_at'] if x['created_at'] else datetime.min, reverse=True)\n",
    "\n",
    "print(f\"\\n🎯 Best Qwen instruct/chat models (sorted by release date, newest first):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show top 50\n",
    "for i, model in enumerate(good_qwen_models[:50], 1):\n",
    "    created_date = model['created_at'].strftime('%Y-%m-%d') if model['created_at'] else 'Unknown'\n",
    "    \n",
    "    print(f\"{i:2d}. 📦 {model['id']}\")\n",
    "    print(f\"    📅 Released: {created_date}\")\n",
    "    print(f\"    ⬇️  Downloads: {model['downloads']:,} | ❤️  Likes: {model['likes']}\")\n",
    "    \n",
    "    # Show model size info from model name\n",
    "    size_indicators = ['0.5B', '1.5B', '3B', '7B', '14B', '32B', '72B', '110B']\n",
    "    model_size = \"Unknown size\"\n",
    "    for size in size_indicators:\n",
    "        if size in model['id']:\n",
    "            model_size = size\n",
    "            break\n",
    "    print(f\"    📏 Size: {model_size}\")\n",
    "    \n",
    "    # Show if it has inference tags\n",
    "    if model['tags']:\n",
    "        inference_tags = [tag for tag in model['tags'] if \n",
    "                         any(keyword in tag.lower() for keyword in \n",
    "                             ['inference', 'compatible', 'endpoints'])]\n",
    "        if inference_tags:\n",
    "            print(f\"    🤖 Inference: {', '.join(inference_tags)}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Extract just the model IDs for testing\n",
    "model_ids_for_testing = [model['id'] for model in good_qwen_models[:10]]  # Top 10 newest\n",
    "\n",
    "print(f\"🧪 Testing top 10 newest Qwen models...\")\n",
    "print(\"Models to test:\", model_ids_for_testing)\n",
    "\n",
    "# Test these models\n",
    "if model_ids_for_testing:\n",
    "    result = check_model_supported(model_ids_for_testing)\n",
    "    \n",
    "    print(f\"\\n✅ Working Qwen models ({result['correct_count']} out of {result['total_tested']}):\")\n",
    "    for model in result['correct_models']:\n",
    "        # Find the release date for this model\n",
    "        model_info = next((m for m in good_qwen_models if m['id'] == model), None)\n",
    "        if model_info:\n",
    "            release_date = model_info['created_at'].strftime('%Y-%m-%d') if model_info['created_at'] else 'Unknown'\n",
    "            print(f\"  - {model} (Released: {release_date})\")\n",
    "        else:\n",
    "            print(f\"  - {model}\")\n",
    "\n",
    "# Show summary by release month\n",
    "print(f\"\\n📊 Release timeline summary:\")\n",
    "from collections import defaultdict\n",
    "\n",
    "monthly_counts = defaultdict(int)\n",
    "for model in good_qwen_models:\n",
    "    if model['created_at']:\n",
    "        month_key = model['created_at'].strftime('%Y-%m')\n",
    "        monthly_counts[month_key] += 1\n",
    "\n",
    "sorted_months = sorted(monthly_counts.items(), reverse=True)\n",
    "for month, count in sorted_months[:12]:  # Last 12 months\n",
    "    print(f\"  {month}: {count} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eebcb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Qwen text-generation models...\n",
      "Preparing to test top 167 Qwen models...\n",
      "Testing models for HuggingFace support...\n",
      "Testing 167 HuggingFace model(s)...\n",
      "==================================================\n",
      "Testing: Qwen/Qwen3-4B-Instruct-2507-FP8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-4B-Instruct-2507-FP8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-4B-Instruct-2507\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-30B-A3B-Instruct-2507-FP8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-30B-A3B-Instruct-2507' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-Coder-480B-A35B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-235B-A22B-Instruct-2507-FP8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct-1M\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-7B-Instruct-1M' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-14B-Instruct-1M\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-14B-Instruct-1M' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-3B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-3B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-3B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-3B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-14B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-14B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-14B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-32B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-32B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-32B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-32B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-14B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-14B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-3B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-0.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-7B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Math-7B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Math-7B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-7B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Coder-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-7B-Instruct-MLX\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-7B-Instruct-MLX' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-1.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-3B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-3B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-72B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-72B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-32B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-32B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-14B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-14B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-7B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-3B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-3B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-1.5B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-1.5B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-0.5B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-0.5B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-72B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-72B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-32B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-32B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-14B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-14B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-7B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-3B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-3B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-1.5B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-1.5B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-0.5B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-0.5B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-32B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Math-72B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Math-72B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-Math-1.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-Math-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-72B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-14B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-14B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-0.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-0.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-Math-1.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-Math-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-Math-7B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-Math-7B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-Math-72B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-Math-72B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-57B-A14B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-57B-A14B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-1.5B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-1.5B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-7B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-7B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-72B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-72B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-1.5B-Instruct-MLX\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-1.5B-Instruct-MLX' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-0.5B-Instruct-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-0.5B-Instruct-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-0.5B-Instruct-MLX\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-0.5B-Instruct-MLX' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-0.5B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-0.5B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-0.5B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-0.5B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-0.5B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-0.5B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-7B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-7B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-7B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-7B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-7B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-7B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-1.5B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-1.5B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-1.5B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-1.5B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-1.5B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-1.5B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-7B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-7B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-57B-A14B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-57B-A14B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-72B-Instruct-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-72B-Instruct-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-72B-Instruct-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-72B-Instruct-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-72B-Instruct-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-72B-Instruct-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-1.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-0.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-0.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-72B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen1.5-110B-Chat-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-110B-Chat-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-110B-Chat-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-110B-Chat-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-110B-Chat-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-110B-Chat-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-110B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-110B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/CodeQwen1.5-7B-Chat-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/CodeQwen1.5-7B-Chat-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/CodeQwen1.5-7B-Chat-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/CodeQwen1.5-7B-Chat-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/CodeQwen1.5-7B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/CodeQwen1.5-7B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-32B-Chat-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-32B-Chat-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-32B-Chat-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-32B-Chat-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-32B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-32B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-32B-Chat-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-32B-Chat-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-MoE-A2.7B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-MoE-A2.7B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-4B-Chat-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-4B-Chat-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-4B-Chat-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-4B-Chat-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-7B-Chat-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-7B-Chat-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-7B-Chat-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-14B-Chat-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-14B-Chat-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-14B-Chat-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-72B-Chat-GPTQ-Int4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-72B-Chat-GPTQ-Int4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-72B-Chat-GPTQ-Int8\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-72B-Chat-GPTQ-Int8' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-4B-Chat-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-4B-Chat-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-1.8B-Chat-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-1.8B-Chat-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-0.5B-Chat-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-0.5B-Chat-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-14B-Chat-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-14B-Chat-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-7B-Chat-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-7B-Chat-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-72B-Chat-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-72B-Chat-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-0.5B-Chat-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-0.5B-Chat-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-1.8B-Chat-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-1.8B-Chat-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-4B-Chat-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-4B-Chat-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-7B-Chat-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-7B-Chat-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-14B-Chat-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-14B-Chat-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-72B-Chat-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-72B-Chat-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-0.5B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-0.5B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-72B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-72B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-14B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-14B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-7B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-7B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-4B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-4B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-1.8B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-1.8B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-Audio-Chat\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-Audio-Chat' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-72B-Chat-Int8\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-72B-Chat-Int8' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-72B-Chat-Int4\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-72B-Chat-Int4' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-1_8B-Chat-Int4\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-1_8B-Chat-Int4' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-1_8B-Chat-Int8\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-1_8B-Chat-Int8' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-1_8B-Chat\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-1_8B-Chat' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-72B-Chat\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-72B-Chat' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-14B-Chat-Int8\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-14B-Chat-Int8' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-7B-Chat-Int8\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-7B-Chat-Int8' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-14B-Chat\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-14B-Chat' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-14B-Chat-Int4\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-14B-Chat-Int4' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-VL-Chat-Int4\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-VL-Chat-Int4' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-7B-Chat-Int4\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-7B-Chat-Int4' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-VL-Chat\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-VL-Chat' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen-7B-Chat\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen-7B-Chat' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "==================================================\n",
      "Summary:\n",
      "  Total tested: 167\n",
      "  Supported: 12\n",
      "  Correct answers: 12\n",
      "\n",
      "✅ Working models:\n",
      "  - Qwen/Qwen3-4B-Instruct-2507\n",
      "  - Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "  - Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\n",
      "  - Qwen/Qwen3-Coder-480B-A35B-Instruct\n",
      "  - Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "  - Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "  - Qwen/Qwen2.5-Coder-3B-Instruct\n",
      "  - Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "  - Qwen/Qwen2.5-32B-Instruct\n",
      "  - Qwen/Qwen2.5-72B-Instruct\n",
      "  - Qwen/Qwen2.5-7B-Instruct\n",
      "  - Qwen/Qwen2-72B-Instruct\n",
      "\n",
      "🎯 Qwen Instruct/Chat Models Summary:\n",
      "Total models: 167\n",
      "Supported models: 12\n",
      "Models with known parameters: 164\n",
      "\n",
      "Top 100 Qwen Models (sorted by release date, newest first):\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "parameters",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "released",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "downloads",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "supported",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "21993241-6dfd-46b7-ae1d-3c238791611c",
       "rows": [
        [
         "0",
         "Qwen3-4B-Instruct-2507-FP8",
         "https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507-FP8",
         "4000000000.0",
         "2025-08-06",
         "5219",
         "False"
        ],
        [
         "1",
         "Qwen3-4B-Instruct-2507",
         "https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507",
         "4000000000.0",
         "2025-08-05",
         "140077",
         "True"
        ],
        [
         "3",
         "Qwen3-Coder-30B-A3B-Instruct",
         "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
         "30000000000.0",
         "2025-07-31",
         "202919",
         "True"
        ],
        [
         "2",
         "Qwen3-Coder-30B-A3B-Instruct-FP8",
         "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8",
         "30000000000.0",
         "2025-07-31",
         "38028",
         "False"
        ],
        [
         "5",
         "Qwen3-30B-A3B-Instruct-2507",
         "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
         "30000000000.0",
         "2025-07-28",
         "446351",
         "False"
        ],
        [
         "4",
         "Qwen3-30B-A3B-Instruct-2507-FP8",
         "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
         "30000000000.0",
         "2025-07-28",
         "74776",
         "False"
        ],
        [
         "6",
         "Qwen3-Coder-480B-A35B-Instruct-FP8",
         "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
         "480000000000.0",
         "2025-07-22",
         "112368",
         "True"
        ],
        [
         "7",
         "Qwen3-Coder-480B-A35B-Instruct",
         "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
         "480000000000.0",
         "2025-07-22",
         "59794",
         "True"
        ],
        [
         "8",
         "Qwen3-235B-A22B-Instruct-2507-FP8",
         "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507-FP8",
         "235000000000.0",
         "2025-07-21",
         "78554",
         "False"
        ],
        [
         "9",
         "Qwen3-235B-A22B-Instruct-2507",
         "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
         "235000000000.0",
         "2025-07-21",
         "71540",
         "True"
        ],
        [
         "10",
         "Qwen2.5-7B-Instruct-1M",
         "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M",
         "7000000000.0",
         "2025-01-23",
         "1419379",
         "False"
        ],
        [
         "11",
         "Qwen2.5-14B-Instruct-1M",
         "https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M",
         "14000000000.0",
         "2025-01-23",
         "11933",
         "False"
        ],
        [
         "19",
         "Qwen2.5-Coder-32B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-AWQ",
         "32000000000.0",
         "2024-11-09",
         "144906",
         "False"
        ],
        [
         "17",
         "Qwen2.5-Coder-14B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ",
         "14000000000.0",
         "2024-11-09",
         "114249",
         "False"
        ],
        [
         "18",
         "Qwen2.5-Coder-32B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF",
         "32000000000.0",
         "2024-11-09",
         "26168",
         "False"
        ],
        [
         "20",
         "Qwen2.5-Coder-32B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4",
         "32000000000.0",
         "2024-11-09",
         "25097",
         "False"
        ],
        [
         "15",
         "Qwen2.5-Coder-3B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-AWQ",
         "3000000000.0",
         "2024-11-09",
         "15216",
         "False"
        ],
        [
         "16",
         "Qwen2.5-Coder-14B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF",
         "14000000000.0",
         "2024-11-09",
         "9520",
         "False"
        ],
        [
         "21",
         "Qwen2.5-Coder-32B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8",
         "32000000000.0",
         "2024-11-09",
         "5809",
         "False"
        ],
        [
         "14",
         "Qwen2.5-Coder-3B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF",
         "3000000000.0",
         "2024-11-09",
         "5080",
         "False"
        ],
        [
         "12",
         "Qwen2.5-Coder-0.5B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF",
         "500000000.0",
         "2024-11-09",
         "4161",
         "False"
        ],
        [
         "22",
         "Qwen2.5-Coder-14B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4",
         "14000000000.0",
         "2024-11-09",
         "2405",
         "False"
        ],
        [
         "23",
         "Qwen2.5-Coder-14B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8",
         "14000000000.0",
         "2024-11-09",
         "1048",
         "False"
        ],
        [
         "24",
         "Qwen2.5-Coder-3B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4",
         "3000000000.0",
         "2024-11-09",
         "1015",
         "False"
        ],
        [
         "13",
         "Qwen2.5-Coder-0.5B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ",
         "500000000.0",
         "2024-11-09",
         "938",
         "False"
        ],
        [
         "27",
         "Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8",
         "500000000.0",
         "2024-11-09",
         "910",
         "False"
        ],
        [
         "25",
         "Qwen2.5-Coder-3B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8",
         "3000000000.0",
         "2024-11-09",
         "113",
         "False"
        ],
        [
         "26",
         "Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4",
         "500000000.0",
         "2024-11-09",
         "78",
         "False"
        ],
        [
         "30",
         "Qwen2.5-Coder-3B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct",
         "3000000000.0",
         "2024-11-06",
         "150862",
         "True"
        ],
        [
         "28",
         "Qwen2.5-Coder-32B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct",
         "32000000000.0",
         "2024-11-06",
         "73622",
         "True"
        ],
        [
         "29",
         "Qwen2.5-Coder-14B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct",
         "14000000000.0",
         "2024-11-06",
         "27198",
         "False"
        ],
        [
         "31",
         "Qwen2.5-Coder-0.5B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct",
         "500000000.0",
         "2024-11-06",
         "17810",
         "False"
        ],
        [
         "33",
         "Qwen2.5-Coder-7B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ",
         "7000000000.0",
         "2024-09-20",
         "89205",
         "False"
        ],
        [
         "35",
         "Qwen2.5-Coder-7B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4",
         "7000000000.0",
         "2024-09-20",
         "1606",
         "False"
        ],
        [
         "32",
         "Qwen2.5-Coder-1.5B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ",
         "1500000000.0",
         "2024-09-20",
         "1500",
         "False"
        ],
        [
         "34",
         "Qwen2.5-Coder-7B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8",
         "7000000000.0",
         "2024-09-20",
         "1023",
         "False"
        ],
        [
         "36",
         "Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8",
         "1500000000.0",
         "2024-09-20",
         "878",
         "False"
        ],
        [
         "37",
         "Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4",
         "1500000000.0",
         "2024-09-20",
         "845",
         "False"
        ],
        [
         "38",
         "Qwen2.5-Math-7B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Math-7B-Instruct",
         "7000000000.0",
         "2024-09-19",
         "71692",
         "False"
        ],
        [
         "40",
         "Qwen2.5-Coder-7B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF",
         "7000000000.0",
         "2024-09-18",
         "44582",
         "False"
        ],
        [
         "41",
         "Qwen2.5-Coder-1.5B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct",
         "1500000000.0",
         "2024-09-18",
         "38428",
         "False"
        ],
        [
         "39",
         "Qwen2.5-Coder-1.5B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF",
         "1500000000.0",
         "2024-09-18",
         "2582",
         "False"
        ],
        [
         "42",
         "Qwen2-7B-Instruct-MLX",
         "https://huggingface.co/Qwen/Qwen2-7B-Instruct-MLX",
         "7000000000.0",
         "2024-09-18",
         "880",
         "False"
        ],
        [
         "43",
         "Qwen2.5-1.5B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct",
         "1500000000.0",
         "2024-09-17",
         "2998900",
         "False"
        ],
        [
         "44",
         "Qwen2.5-3B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct",
         "3000000000.0",
         "2024-09-17",
         "1212575",
         "False"
        ],
        [
         "74",
         "Qwen2.5-32B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct",
         "32000000000.0",
         "2024-09-17",
         "346916",
         "True"
        ],
        [
         "59",
         "Qwen2.5-Coder-7B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct",
         "7000000000.0",
         "2024-09-17",
         "244138",
         "True"
        ],
        [
         "63",
         "Qwen2.5-32B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4",
         "32000000000.0",
         "2024-09-17",
         "156950",
         "False"
        ],
        [
         "62",
         "Qwen2.5-32B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8",
         "32000000000.0",
         "2024-09-17",
         "138433",
         "False"
        ],
        [
         "52",
         "Qwen2.5-72B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-72B-Instruct-AWQ",
         "72000000000.0",
         "2024-09-17",
         "69484",
         "False"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 167
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>url</th>\n",
       "      <th>parameters</th>\n",
       "      <th>released</th>\n",
       "      <th>downloads</th>\n",
       "      <th>supported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen3-4B-Instruct-2507-FP8</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-4B-Instruct-...</td>\n",
       "      <td>4.000000e+09</td>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>5219</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen3-4B-Instruct-2507</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-4B-Instruct-...</td>\n",
       "      <td>4.000000e+09</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>140077</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen3-Coder-30B-A3B-Instruct</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-Coder-30B-A3...</td>\n",
       "      <td>3.000000e+10</td>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>202919</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen3-Coder-30B-A3B-Instruct-FP8</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-Coder-30B-A3...</td>\n",
       "      <td>3.000000e+10</td>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>38028</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qwen3-30B-A3B-Instruct-2507</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-30B-A3B-Inst...</td>\n",
       "      <td>3.000000e+10</td>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>446351</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Qwen-14B-Chat-Int4</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-14B-Chat-Int4</td>\n",
       "      <td>1.400000e+10</td>\n",
       "      <td>2023-09-24</td>\n",
       "      <td>912</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Qwen-VL-Chat-Int4</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-VL-Chat-Int4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>2651</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Qwen-VL-Chat</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-VL-Chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-20</td>\n",
       "      <td>70651</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Qwen-7B-Chat-Int4</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-7B-Chat-Int4</td>\n",
       "      <td>7.000000e+09</td>\n",
       "      <td>2023-08-20</td>\n",
       "      <td>1658</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Qwen-7B-Chat</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-7B-Chat</td>\n",
       "      <td>7.000000e+09</td>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>190098</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  \\\n",
       "0          Qwen3-4B-Instruct-2507-FP8   \n",
       "1              Qwen3-4B-Instruct-2507   \n",
       "3        Qwen3-Coder-30B-A3B-Instruct   \n",
       "2    Qwen3-Coder-30B-A3B-Instruct-FP8   \n",
       "5         Qwen3-30B-A3B-Instruct-2507   \n",
       "..                                ...   \n",
       "162                Qwen-14B-Chat-Int4   \n",
       "163                 Qwen-VL-Chat-Int4   \n",
       "165                      Qwen-VL-Chat   \n",
       "164                 Qwen-7B-Chat-Int4   \n",
       "166                      Qwen-7B-Chat   \n",
       "\n",
       "                                                   url    parameters  \\\n",
       "0    https://huggingface.co/Qwen/Qwen3-4B-Instruct-...  4.000000e+09   \n",
       "1    https://huggingface.co/Qwen/Qwen3-4B-Instruct-...  4.000000e+09   \n",
       "3    https://huggingface.co/Qwen/Qwen3-Coder-30B-A3...  3.000000e+10   \n",
       "2    https://huggingface.co/Qwen/Qwen3-Coder-30B-A3...  3.000000e+10   \n",
       "5    https://huggingface.co/Qwen/Qwen3-30B-A3B-Inst...  3.000000e+10   \n",
       "..                                                 ...           ...   \n",
       "162     https://huggingface.co/Qwen/Qwen-14B-Chat-Int4  1.400000e+10   \n",
       "163      https://huggingface.co/Qwen/Qwen-VL-Chat-Int4           NaN   \n",
       "165           https://huggingface.co/Qwen/Qwen-VL-Chat           NaN   \n",
       "164      https://huggingface.co/Qwen/Qwen-7B-Chat-Int4  7.000000e+09   \n",
       "166           https://huggingface.co/Qwen/Qwen-7B-Chat  7.000000e+09   \n",
       "\n",
       "       released  downloads  supported  \n",
       "0    2025-08-06       5219      False  \n",
       "1    2025-08-05     140077       True  \n",
       "3    2025-07-31     202919       True  \n",
       "2    2025-07-31      38028      False  \n",
       "5    2025-07-28     446351      False  \n",
       "..          ...        ...        ...  \n",
       "162  2023-09-24        912      False  \n",
       "163  2023-08-31       2651      False  \n",
       "165  2023-08-20      70651      False  \n",
       "164  2023-08-20       1658      False  \n",
       "166  2023-08-03     190098      False  \n",
       "\n",
       "[167 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Summary Statistics:\n",
      "✅ Supported models: 12 out of 167 (7.2%)\n",
      "📏 Parameter sizes:\n",
      "   0.5B: 21 models\n",
      "   1.5B: 18 models\n",
      "   1.8B: 5 models\n",
      "   2.7B: 2 models\n",
      "   3B: 10 models\n",
      "   4B: 7 models\n",
      "   7B: 30 models\n",
      "   8B: 3 models\n",
      "   14B: 19 models\n",
      "   30B: 4 models\n",
      "   32B: 14 models\n",
      "   57B: 3 models\n",
      "   72B: 20 models\n",
      "   110B: 4 models\n",
      "   235B: 2 models\n",
      "   480B: 2 models\n",
      "📅 Release date range: 2023-08-03 to 2025-08-06\n",
      "⬇️  Download range: 13 to 12,195,236\n",
      "\n",
      "✅ Working Models (12):\n",
      "   - Qwen3-4B-Instruct-2507 (4B, 140,077 downloads)\n",
      "   - Qwen3-Coder-30B-A3B-Instruct (30B, 202,919 downloads)\n",
      "   - Qwen3-Coder-480B-A35B-Instruct-FP8 (480B, 112,368 downloads)\n",
      "   - Qwen3-Coder-480B-A35B-Instruct (480B, 59,794 downloads)\n",
      "   - Qwen3-235B-A22B-Instruct-2507 (235B, 71,540 downloads)\n",
      "   - Qwen2.5-Coder-3B-Instruct (3B, 150,862 downloads)\n",
      "   - Qwen2.5-Coder-32B-Instruct (32B, 73,622 downloads)\n",
      "   - Qwen2.5-32B-Instruct (32B, 346,916 downloads)\n",
      "   - Qwen2.5-Coder-7B-Instruct (7B, 244,138 downloads)\n",
      "   - Qwen2.5-7B-Instruct (7B, 9,128,290 downloads)\n",
      "   - Qwen2.5-72B-Instruct (72B, 101,682 downloads)\n",
      "   - Qwen2-72B-Instruct (72B, 39,268 downloads)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "parameters",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "released",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "downloads",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "supported",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "89468f11-9bb8-4ca0-86e4-aeba6e9ba38b",
       "rows": [
        [
         "0",
         "Qwen3-4B-Instruct-2507-FP8",
         "https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507-FP8",
         "4000000000.0",
         "2025-08-06",
         "5219",
         "False"
        ],
        [
         "1",
         "Qwen3-4B-Instruct-2507",
         "https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507",
         "4000000000.0",
         "2025-08-05",
         "140077",
         "True"
        ],
        [
         "3",
         "Qwen3-Coder-30B-A3B-Instruct",
         "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
         "30000000000.0",
         "2025-07-31",
         "202919",
         "True"
        ],
        [
         "2",
         "Qwen3-Coder-30B-A3B-Instruct-FP8",
         "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8",
         "30000000000.0",
         "2025-07-31",
         "38028",
         "False"
        ],
        [
         "5",
         "Qwen3-30B-A3B-Instruct-2507",
         "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
         "30000000000.0",
         "2025-07-28",
         "446351",
         "False"
        ],
        [
         "4",
         "Qwen3-30B-A3B-Instruct-2507-FP8",
         "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
         "30000000000.0",
         "2025-07-28",
         "74776",
         "False"
        ],
        [
         "6",
         "Qwen3-Coder-480B-A35B-Instruct-FP8",
         "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
         "480000000000.0",
         "2025-07-22",
         "112368",
         "True"
        ],
        [
         "7",
         "Qwen3-Coder-480B-A35B-Instruct",
         "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
         "480000000000.0",
         "2025-07-22",
         "59794",
         "True"
        ],
        [
         "8",
         "Qwen3-235B-A22B-Instruct-2507-FP8",
         "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507-FP8",
         "235000000000.0",
         "2025-07-21",
         "78554",
         "False"
        ],
        [
         "9",
         "Qwen3-235B-A22B-Instruct-2507",
         "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
         "235000000000.0",
         "2025-07-21",
         "71540",
         "True"
        ],
        [
         "10",
         "Qwen2.5-7B-Instruct-1M",
         "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M",
         "7000000000.0",
         "2025-01-23",
         "1419379",
         "False"
        ],
        [
         "11",
         "Qwen2.5-14B-Instruct-1M",
         "https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M",
         "14000000000.0",
         "2025-01-23",
         "11933",
         "False"
        ],
        [
         "19",
         "Qwen2.5-Coder-32B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-AWQ",
         "32000000000.0",
         "2024-11-09",
         "144906",
         "False"
        ],
        [
         "17",
         "Qwen2.5-Coder-14B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ",
         "14000000000.0",
         "2024-11-09",
         "114249",
         "False"
        ],
        [
         "18",
         "Qwen2.5-Coder-32B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF",
         "32000000000.0",
         "2024-11-09",
         "26168",
         "False"
        ],
        [
         "20",
         "Qwen2.5-Coder-32B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4",
         "32000000000.0",
         "2024-11-09",
         "25097",
         "False"
        ],
        [
         "15",
         "Qwen2.5-Coder-3B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-AWQ",
         "3000000000.0",
         "2024-11-09",
         "15216",
         "False"
        ],
        [
         "16",
         "Qwen2.5-Coder-14B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF",
         "14000000000.0",
         "2024-11-09",
         "9520",
         "False"
        ],
        [
         "21",
         "Qwen2.5-Coder-32B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8",
         "32000000000.0",
         "2024-11-09",
         "5809",
         "False"
        ],
        [
         "14",
         "Qwen2.5-Coder-3B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF",
         "3000000000.0",
         "2024-11-09",
         "5080",
         "False"
        ],
        [
         "12",
         "Qwen2.5-Coder-0.5B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF",
         "500000000.0",
         "2024-11-09",
         "4161",
         "False"
        ],
        [
         "22",
         "Qwen2.5-Coder-14B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4",
         "14000000000.0",
         "2024-11-09",
         "2405",
         "False"
        ],
        [
         "23",
         "Qwen2.5-Coder-14B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8",
         "14000000000.0",
         "2024-11-09",
         "1048",
         "False"
        ],
        [
         "24",
         "Qwen2.5-Coder-3B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4",
         "3000000000.0",
         "2024-11-09",
         "1015",
         "False"
        ],
        [
         "13",
         "Qwen2.5-Coder-0.5B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ",
         "500000000.0",
         "2024-11-09",
         "938",
         "False"
        ],
        [
         "27",
         "Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8",
         "500000000.0",
         "2024-11-09",
         "910",
         "False"
        ],
        [
         "25",
         "Qwen2.5-Coder-3B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8",
         "3000000000.0",
         "2024-11-09",
         "113",
         "False"
        ],
        [
         "26",
         "Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4",
         "500000000.0",
         "2024-11-09",
         "78",
         "False"
        ],
        [
         "30",
         "Qwen2.5-Coder-3B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct",
         "3000000000.0",
         "2024-11-06",
         "150862",
         "True"
        ],
        [
         "28",
         "Qwen2.5-Coder-32B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct",
         "32000000000.0",
         "2024-11-06",
         "73622",
         "True"
        ],
        [
         "29",
         "Qwen2.5-Coder-14B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct",
         "14000000000.0",
         "2024-11-06",
         "27198",
         "False"
        ],
        [
         "31",
         "Qwen2.5-Coder-0.5B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct",
         "500000000.0",
         "2024-11-06",
         "17810",
         "False"
        ],
        [
         "33",
         "Qwen2.5-Coder-7B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ",
         "7000000000.0",
         "2024-09-20",
         "89205",
         "False"
        ],
        [
         "35",
         "Qwen2.5-Coder-7B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4",
         "7000000000.0",
         "2024-09-20",
         "1606",
         "False"
        ],
        [
         "32",
         "Qwen2.5-Coder-1.5B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ",
         "1500000000.0",
         "2024-09-20",
         "1500",
         "False"
        ],
        [
         "34",
         "Qwen2.5-Coder-7B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8",
         "7000000000.0",
         "2024-09-20",
         "1023",
         "False"
        ],
        [
         "36",
         "Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8",
         "1500000000.0",
         "2024-09-20",
         "878",
         "False"
        ],
        [
         "37",
         "Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4",
         "1500000000.0",
         "2024-09-20",
         "845",
         "False"
        ],
        [
         "38",
         "Qwen2.5-Math-7B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Math-7B-Instruct",
         "7000000000.0",
         "2024-09-19",
         "71692",
         "False"
        ],
        [
         "40",
         "Qwen2.5-Coder-7B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF",
         "7000000000.0",
         "2024-09-18",
         "44582",
         "False"
        ],
        [
         "41",
         "Qwen2.5-Coder-1.5B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct",
         "1500000000.0",
         "2024-09-18",
         "38428",
         "False"
        ],
        [
         "39",
         "Qwen2.5-Coder-1.5B-Instruct-GGUF",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF",
         "1500000000.0",
         "2024-09-18",
         "2582",
         "False"
        ],
        [
         "42",
         "Qwen2-7B-Instruct-MLX",
         "https://huggingface.co/Qwen/Qwen2-7B-Instruct-MLX",
         "7000000000.0",
         "2024-09-18",
         "880",
         "False"
        ],
        [
         "43",
         "Qwen2.5-1.5B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct",
         "1500000000.0",
         "2024-09-17",
         "2998900",
         "False"
        ],
        [
         "44",
         "Qwen2.5-3B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct",
         "3000000000.0",
         "2024-09-17",
         "1212575",
         "False"
        ],
        [
         "74",
         "Qwen2.5-32B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct",
         "32000000000.0",
         "2024-09-17",
         "346916",
         "True"
        ],
        [
         "59",
         "Qwen2.5-Coder-7B-Instruct",
         "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct",
         "7000000000.0",
         "2024-09-17",
         "244138",
         "True"
        ],
        [
         "63",
         "Qwen2.5-32B-Instruct-GPTQ-Int4",
         "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4",
         "32000000000.0",
         "2024-09-17",
         "156950",
         "False"
        ],
        [
         "62",
         "Qwen2.5-32B-Instruct-GPTQ-Int8",
         "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8",
         "32000000000.0",
         "2024-09-17",
         "138433",
         "False"
        ],
        [
         "52",
         "Qwen2.5-72B-Instruct-AWQ",
         "https://huggingface.co/Qwen/Qwen2.5-72B-Instruct-AWQ",
         "72000000000.0",
         "2024-09-17",
         "69484",
         "False"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 167
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>url</th>\n",
       "      <th>parameters</th>\n",
       "      <th>released</th>\n",
       "      <th>downloads</th>\n",
       "      <th>supported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen3-4B-Instruct-2507-FP8</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-4B-Instruct-...</td>\n",
       "      <td>4.000000e+09</td>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>5219</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen3-4B-Instruct-2507</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-4B-Instruct-...</td>\n",
       "      <td>4.000000e+09</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>140077</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen3-Coder-30B-A3B-Instruct</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-Coder-30B-A3...</td>\n",
       "      <td>3.000000e+10</td>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>202919</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen3-Coder-30B-A3B-Instruct-FP8</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-Coder-30B-A3...</td>\n",
       "      <td>3.000000e+10</td>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>38028</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qwen3-30B-A3B-Instruct-2507</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen3-30B-A3B-Inst...</td>\n",
       "      <td>3.000000e+10</td>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>446351</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Qwen-14B-Chat-Int4</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-14B-Chat-Int4</td>\n",
       "      <td>1.400000e+10</td>\n",
       "      <td>2023-09-24</td>\n",
       "      <td>912</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Qwen-VL-Chat-Int4</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-VL-Chat-Int4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>2651</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Qwen-VL-Chat</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-VL-Chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-20</td>\n",
       "      <td>70651</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Qwen-7B-Chat-Int4</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-7B-Chat-Int4</td>\n",
       "      <td>7.000000e+09</td>\n",
       "      <td>2023-08-20</td>\n",
       "      <td>1658</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Qwen-7B-Chat</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen-7B-Chat</td>\n",
       "      <td>7.000000e+09</td>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>190098</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  \\\n",
       "0          Qwen3-4B-Instruct-2507-FP8   \n",
       "1              Qwen3-4B-Instruct-2507   \n",
       "3        Qwen3-Coder-30B-A3B-Instruct   \n",
       "2    Qwen3-Coder-30B-A3B-Instruct-FP8   \n",
       "5         Qwen3-30B-A3B-Instruct-2507   \n",
       "..                                ...   \n",
       "162                Qwen-14B-Chat-Int4   \n",
       "163                 Qwen-VL-Chat-Int4   \n",
       "165                      Qwen-VL-Chat   \n",
       "164                 Qwen-7B-Chat-Int4   \n",
       "166                      Qwen-7B-Chat   \n",
       "\n",
       "                                                   url    parameters  \\\n",
       "0    https://huggingface.co/Qwen/Qwen3-4B-Instruct-...  4.000000e+09   \n",
       "1    https://huggingface.co/Qwen/Qwen3-4B-Instruct-...  4.000000e+09   \n",
       "3    https://huggingface.co/Qwen/Qwen3-Coder-30B-A3...  3.000000e+10   \n",
       "2    https://huggingface.co/Qwen/Qwen3-Coder-30B-A3...  3.000000e+10   \n",
       "5    https://huggingface.co/Qwen/Qwen3-30B-A3B-Inst...  3.000000e+10   \n",
       "..                                                 ...           ...   \n",
       "162     https://huggingface.co/Qwen/Qwen-14B-Chat-Int4  1.400000e+10   \n",
       "163      https://huggingface.co/Qwen/Qwen-VL-Chat-Int4           NaN   \n",
       "165           https://huggingface.co/Qwen/Qwen-VL-Chat           NaN   \n",
       "164      https://huggingface.co/Qwen/Qwen-7B-Chat-Int4  7.000000e+09   \n",
       "166           https://huggingface.co/Qwen/Qwen-7B-Chat  7.000000e+09   \n",
       "\n",
       "       released  downloads  supported  \n",
       "0    2025-08-06       5219      False  \n",
       "1    2025-08-05     140077       True  \n",
       "3    2025-07-31     202919       True  \n",
       "2    2025-07-31      38028      False  \n",
       "5    2025-07-28     446351      False  \n",
       "..          ...        ...        ...  \n",
       "162  2023-09-24        912      False  \n",
       "163  2023-08-31       2651      False  \n",
       "165  2023-08-20      70651      False  \n",
       "164  2023-08-20       1658      False  \n",
       "166  2023-08-03     190098      False  \n",
       "\n",
       "[167 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "def extract_parameters(model_name):\n",
    "    \"\"\"Extract number of parameters from model name and convert to integer\"\"\"\n",
    "    # Look for patterns like 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B, 110B\n",
    "    pattern = r'(\\d+(?:\\.\\d+)?)[Bb]'\n",
    "    match = re.search(pattern, model_name)\n",
    "    \n",
    "    if match:\n",
    "        size_str = match.group(1)\n",
    "        size_float = float(size_str)\n",
    "        # Convert to actual number (B = billion)\n",
    "        return int(size_float * 1_000_000_000)\n",
    "    \n",
    "    return None  # Unknown size\n",
    "\n",
    "# Get Qwen text-generation models\n",
    "print(\"Fetching Qwen text-generation models...\")\n",
    "qwen_models = api.list_models(\n",
    "    author=\"Qwen\", \n",
    "    task=\"text-generation\",\n",
    "    limit=None,\n",
    "    sort=\"lastModified\",\n",
    "    direction=-1\n",
    ")\n",
    "\n",
    "# Filter for likely chat/instruct models\n",
    "good_qwen_models = []\n",
    "\n",
    "for model in qwen_models:\n",
    "    # Look for instruct/chat indicators\n",
    "    is_instruct = any(keyword in model.id.lower() for keyword in [\n",
    "        'instruct', 'chat', 'conversation'\n",
    "    ])\n",
    "    \n",
    "    # Check if it has good stats and is public\n",
    "    if is_instruct and not model.private and model.downloads > 5:\n",
    "        good_qwen_models.append({\n",
    "            'id': model.id,\n",
    "            'created_at': model.created_at,\n",
    "            'downloads': model.downloads,\n",
    "            'likes': model.likes,\n",
    "            'tags': model.tags\n",
    "        })\n",
    "\n",
    "# Sort by creation date (newest first)\n",
    "good_qwen_models.sort(key=lambda x: x['created_at'] if x['created_at'] else datetime.min, reverse=True)\n",
    "\n",
    "# Take top 100\n",
    "top_100_models = good_qwen_models # good_qwen_models[:100]\n",
    "\n",
    "print(f\"Preparing to test top {len(top_100_models)} Qwen models...\")\n",
    "\n",
    "# Test all models for support\n",
    "model_ids_for_testing = [model['id'] for model in top_100_models]\n",
    "print(\"Testing models for HuggingFace support...\")\n",
    "\n",
    "# Test the models (this might take a while)\n",
    "support_result = check_model_supported(model_ids_for_testing)\n",
    "\n",
    "# Create DataFrame\n",
    "df_data = []\n",
    "\n",
    "for model in top_100_models:\n",
    "    model_name = model['id']\n",
    "    \n",
    "    # Extract just the model name (remove \"Qwen/\" prefix)\n",
    "    display_name = model_name.split('/')[-1]\n",
    "    \n",
    "    # Create URL\n",
    "    url = f\"https://huggingface.co/{model_name}\"\n",
    "    \n",
    "    # Extract parameters\n",
    "    parameters = extract_parameters(model_name)\n",
    "    \n",
    "    # Format release date\n",
    "    released = model['created_at'].strftime('%Y-%m-%d') if model['created_at'] else None\n",
    "    \n",
    "    # Downloads\n",
    "    downloads = model['downloads']\n",
    "    \n",
    "    # Check if supported\n",
    "    supported = model_name in support_result['correct_models']\n",
    "    \n",
    "    df_data.append({\n",
    "        'model_name': display_name,\n",
    "        'url': url,\n",
    "        'parameters': parameters,\n",
    "        'released': released,\n",
    "        'downloads': downloads,\n",
    "        'supported': supported\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "# Sort by release date (newest first) and then by downloads\n",
    "df['released_dt'] = pd.to_datetime(df['released'])\n",
    "df = df.sort_values(['released_dt', 'downloads'], ascending=[False, False])\n",
    "\n",
    "# Drop the helper column\n",
    "df = df.drop('released_dt', axis=1)\n",
    "\n",
    "print(f\"\\n🎯 Qwen Instruct/Chat Models Summary:\")\n",
    "print(f\"Total models: {len(df)}\")\n",
    "print(f\"Supported models: {df['supported'].sum()}\")\n",
    "print(f\"Models with known parameters: {df['parameters'].notna().sum()}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(f\"\\nTop 100 Qwen Models (sorted by release date, newest first):\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Format the display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Show the dataframe\n",
    "display(df)\n",
    "\n",
    "# Additional summary statistics\n",
    "print(f\"\\n📊 Summary Statistics:\")\n",
    "print(f\"✅ Supported models: {df['supported'].sum()} out of {len(df)} ({df['supported'].mean()*100:.1f}%)\")\n",
    "\n",
    "if df['parameters'].notna().any():\n",
    "    print(f\"📏 Parameter sizes:\")\n",
    "    param_summary = df[df['parameters'].notna()]['parameters'].value_counts().sort_index()\n",
    "    for params, count in param_summary.items():\n",
    "        size_b = params / 1_000_000_000\n",
    "        print(f\"   {size_b:g}B: {count} models\")\n",
    "\n",
    "print(f\"📅 Release date range: {df['released'].min()} to {df['released'].max()}\")\n",
    "print(f\"⬇️  Download range: {df['downloads'].min():,} to {df['downloads'].max():,}\")\n",
    "\n",
    "# Show working models\n",
    "working_models = df[df['supported'] == True]\n",
    "if not working_models.empty:\n",
    "    print(f\"\\n✅ Working Models ({len(working_models)}):\")\n",
    "    for _, row in working_models.iterrows():\n",
    "        params_str = f\"{row['parameters']/1_000_000_000:g}B\" if pd.notna(row['parameters']) else \"Unknown\"\n",
    "        print(f\"   - {row['model_name']} ({params_str}, {row['downloads']:,} downloads)\")\n",
    "\n",
    "# Return the dataframe for further analysis\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a9712e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = df[df['supported'] == True]\n",
    "working_df.to_csv('qwen_models_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cb203ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m first_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel object type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(first_model))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvailable attributes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "first_model = list(models)[0]\n",
    "print(\"Model object type:\", type(first_model))\n",
    "print(\"\\nAvailable attributes:\")\n",
    "for attr in dir(first_model):\n",
    "    if not attr.startswith('_'):  # Skip private attributes\n",
    "        try:\n",
    "            value = getattr(first_model, attr)\n",
    "            if not callable(value):  # Skip methods\n",
    "                print(f\"  {attr}: {type(value)} = {value}\")\n",
    "        except:\n",
    "            print(f\"  {attr}: <error accessing>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d65ed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 756 promising chat/instruct models:\n",
      "============================================================\n",
      "📦 meta-llama/Llama-3.1-8B-Instruct\n",
      "   ⬇️  13,028,195 downloads, ❤️  4489 likes\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-14B-Instruct\n",
      "   ⬇️  12,195,236 downloads, ❤️  264 likes\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 openai-community/gpt2\n",
      "   ⬇️  10,619,980 downloads, ❤️  2899 likes\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen2.5-7B-Instruct\n",
      "   ⬇️  9,128,290 downloads, ❤️  760 likes\n",
      "   🏷️  ['chat', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 facebook/opt-125m\n",
      "   ⬇️  7,325,030 downloads, ❤️  211 likes\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference']\n",
      "\n",
      "📦 Qwen/Qwen3-4B-Base\n",
      "   ⬇️  5,476,711 downloads, ❤️  50 likes\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 Qwen/Qwen3-8B\n",
      "   ⬇️  5,160,545 downloads, ❤️  544 likes\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 dphn/dolphin-2.9.1-yi-1.5-34b\n",
      "   ⬇️  4,315,252 downloads, ❤️  39 likes\n",
      "   🏷️  ['dataset:m-a-p/CodeFeedback-Filtered-Instruction', 'dataset:Locutusque/function-calling-chatml', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n",
      "📦 openai/gpt-oss-20b\n",
      "   ⬇️  3,653,079 downloads, ❤️  3127 likes\n",
      "   🏷️  ['autotrain_compatible', 'endpoints_compatible']\n",
      "\n",
      "📦 context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16\n",
      "   ⬇️  3,591,932 downloads, ❤️  2 likes\n",
      "   🏷️  ['autotrain_compatible', 'text-generation-inference', 'endpoints_compatible']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Get text-generation models\n",
    "models = api.list_models(\n",
    "    task=\"text-generation\",\n",
    "    limit=1000,\n",
    "    sort=\"downloads\", \n",
    "    direction=-1\n",
    ")\n",
    "\n",
    "# Filter for likely chat/instruct models\n",
    "good_models = []\n",
    "\n",
    "for model in models:\n",
    "    # Check if it's a chat/instruct model\n",
    "    is_chat_model = any(keyword in model.id.lower() for keyword in [\n",
    "        'instruct', 'chat', 'conversation'\n",
    "    ])\n",
    "    \n",
    "    # Check tags for chat/instruct indicators\n",
    "    has_chat_tags = False\n",
    "    if model.tags:\n",
    "        has_chat_tags = any(keyword in ' '.join(model.tags).lower() for keyword in [\n",
    "            'instruct', 'chat', 'conversation', 'text-generation-inference'\n",
    "        ])\n",
    "    \n",
    "    # Filter criteria\n",
    "    if (is_chat_model or has_chat_tags) and \\\n",
    "       model.library_name == 'transformers' and \\\n",
    "       not model.private and \\\n",
    "       model.downloads > 1000:  # Has some usage\n",
    "        \n",
    "        good_models.append({\n",
    "            'id': model.id,\n",
    "            'downloads': model.downloads,\n",
    "            'likes': model.likes,\n",
    "            'tags': model.tags\n",
    "        })\n",
    "\n",
    "# Sort by downloads and show top candidates\n",
    "good_models.sort(key=lambda x: x['downloads'], reverse=True)\n",
    "\n",
    "print(f\"Found {len(good_models)} promising chat/instruct models:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model in good_models[:10]:\n",
    "    print(f\"📦 {model['id']}\")\n",
    "    print(f\"   ⬇️  {model['downloads']:,} downloads, ❤️  {model['likes']} likes\")\n",
    "    \n",
    "    # Show relevant tags\n",
    "    relevant_tags = [tag for tag in model['tags'] if \n",
    "                    any(keyword in tag.lower() for keyword in \n",
    "                        ['instruct', 'chat', 'inference', 'compatible'])]\n",
    "    if relevant_tags:\n",
    "        print(f\"   🏷️  {relevant_tags}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5f3d846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing promising models:\n",
      "['meta-llama/Llama-3.1-8B-Instruct', 'Qwen/Qwen2.5-14B-Instruct', 'openai-community/gpt2', 'Qwen/Qwen2.5-7B-Instruct', 'facebook/opt-125m', 'Qwen/Qwen3-4B-Base', 'Qwen/Qwen3-8B', 'dphn/dolphin-2.9.1-yi-1.5-34b', 'openai/gpt-oss-20b', 'context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16', 'Gensyn/Qwen2.5-0.5B-Instruct', 'meta-llama/Llama-3.2-1B-Instruct', 'distilbert/distilgpt2', 'Qwen/Qwen2.5-1.5B-Instruct', 'Qwen/Qwen3-0.6B', 'openai-community/gpt2-large', 'google/gemma-3-1b-it', 'meta-llama/Llama-3.2-1B', 'trl-internal-testing/tiny-Qwen2ForCausalLM-2.5', 'Qwen/Qwen3-8B-Base', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'petals-team/StableBeluga2', 'meta-llama/Llama-3.2-3B-Instruct', 'Qwen/Qwen2.5-7B-Instruct-1M', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'AIDC-AI/Ovis2-4B', 'Qwen/Qwen3-1.7B', 'Qwen/Qwen3-0.6B-Base', 'Qwen/Qwen2.5-3B-Instruct', 'Qwen/Qwen3-4B', 'google-t5/t5-3b', 'microsoft/DialoGPT-medium', 'Qwen/Qwen2.5-0.5B-Instruct', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', 'lmstudio-community/gpt-oss-20b-MLX-8bit', 'Qwen/Qwen2.5-7B', 'apple/OpenELM-1_1B-Instruct', 'meta-llama/Meta-Llama-3-8B-Instruct', 'Qwen/Qwen3-30B-A3B', 'microsoft/Phi-3-mini-128k-instruct', 'HuggingFaceTB/SmolLM2-135M', 'google/gemma-2-2b', 'Qwen/Qwen3-32B', 'meta-llama/Llama-2-7b-chat-hf', 'Qwen/Qwen2.5-0.5B', 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', 'openai/gpt-oss-120b', 'bigscience/bloomz-560m', 'meta-llama/Meta-Llama-3-8B', 'deepseek-ai/DeepSeek-R1', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'meta-llama/Llama-3.1-8B', 'Qwen/Qwen3-14B', 'google-t5/t5-11b', 'deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct', 'mistralai/Mistral-7B-Instruct-v0.2', 'Qwen/Qwen1.5-0.5B-Chat', 'microsoft/phi-2', 'HuggingFaceH4/zephyr-7b-beta', 'Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24', 'microsoft/phi-4', 'HuggingFaceTB/SmolLM3-3B', 'meta-llama/Llama-2-7b-hf', 'Qwen/Qwen3-32B-AWQ', 'unsloth/gpt-oss-20b-GGUF', 'HuggingFaceTB/SmolLM2-360M-Instruct', 'Qwen/Qwen2-1.5B-Instruct', 'meta-llama/Llama-3.1-70B-Instruct', 'lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit', 'microsoft/Phi-3.5-vision-instruct', 'google/gemma-2-2b-it', 'trl-internal-testing/tiny-random-LlamaForCausalLM', 'lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-8bit', 'microsoft/Phi-3-mini-4k-instruct', 'lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-6bit', 'openai-community/gpt2-medium', 'AIDC-AI/Ovis2-1B', 'deepseek-ai/DeepSeek-V3', 'Qwen/Qwen3-30B-A3B-Instruct-2507', 'microsoft/Phi-4-multimodal-instruct', 'deepseek-ai/DeepSeek-R1-0528', 'meta-llama/Llama-3.3-70B-Instruct', 'deepseek-ai/deepseek-coder-6.7b-base', 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit', 'deepseek-ai/DeepSeek-V3-0324', 'moonshotai/Kimi-K2-Instruct', 'HuggingFaceTB/SmolLM2-135M-Instruct', 'unsloth/mistral-7b-v0.3-bnb-4bit', 'Qwen/Qwen2.5-1.5B', 'mistralai/Mistral-7B-v0.1', 'lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-5bit', 'bigcode/starcoderbase-1b', 'unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF', 'Qwen/Qwen2.5-32B-Instruct', 'deepseek-ai/DeepSeek-R1-0528-Qwen3-8B', 'microsoft/DialoGPT-large', 'meta-llama/Llama-Guard-3-8B', 'lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-MLX-6bit', 'lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-MLX-4bit', 'hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4']\n",
      "Testing 100 HuggingFace model(s)...\n",
      "==================================================\n",
      "Testing: meta-llama/Llama-3.1-8B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-14B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-14B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: openai-community/gpt2\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'openai-community/gpt2' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: facebook/opt-125m\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'facebook/opt-125m' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-4B-Base\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-4B-Base' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-8B\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: dphn/dolphin-2.9.1-yi-1.5-34b\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'dphn/dolphin-2.9.1-yi-1.5-34b' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: openai/gpt-oss-20b\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Gensyn/Qwen2.5-0.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Gensyn/Qwen2.5-0.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: meta-llama/Llama-3.2-1B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: distilbert/distilgpt2\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'distilbert/distilgpt2' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-1.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-0.6B\n",
      "   ❌ FAILED - Unknown error: Error code: 404 - {'error': {'code': 'NOT_FOUND', 'message': 'Model not found, inaccessible, and/or not deployed', 'requestId': 'chatcmpl-a181ee27dcfc424ab35a35e71ae48e69', 'param': 'model'}}\n",
      "\n",
      "Testing: openai-community/gpt2-large\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'openai-community/gpt2-large' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: google/gemma-3-1b-it\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'google/gemma-3-1b-it' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: meta-llama/Llama-3.2-1B\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'meta-llama/Llama-3.2-1B' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: trl-internal-testing/tiny-Qwen2ForCausalLM-2.5\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'trl-internal-testing/tiny-Qwen2ForCausalLM-2.5' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-8B-Base\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-8B-Base' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "   ✅ SUPPORTED - Response: '1 + 1 = \\boxed{2}' (CORRECT ✓)\n",
      "\n",
      "Testing: petals-team/StableBeluga2\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'petals-team/StableBeluga2' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: meta-llama/Llama-3.2-3B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct-1M\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-7B-Instruct-1M' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: AIDC-AI/Ovis2-4B\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'AIDC-AI/Ovis2-4B' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-1.7B\n",
      "   ❌ FAILED - Invalid API key: Error code: 404 - {'error': {'code': 'NOT_FOUND', 'message': 'Model not found, inaccessible, and/or not deployed', 'requestId': 'chatcmpl-35e648dac6374017958e5d7110810343', 'param': 'model'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-0.6B-Base\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-0.6B-Base' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-3B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-3B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-4B\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "Okay, the user is asking me to calculate 1 plus 1. Let me make sure I understand the question correctly. They want the sum of 1 and 1. Since I'm a calculator, my job is to provide the numerical answer without any extra explanation.\n",
      "\n",
      "First, I'll recall the basic arithmetic operation. Adding 1 and 1 together. In simple terms, 1 + 1 equals 2. There's no need for any complex calculations here. The user has specified that they just want the answer, so I should avoid any additional text or reasoning.\n",
      "\n",
      "I need to confirm that there are no hidden tricks or contexts in the question. The user mentioned \"just give me the answer,\" so I should stick strictly to the mathematical operation. No variables, no exponents, just straightforward addition. \n",
      "\n",
      "Also, the user emphasized that I should only output numbers and never show my thinking process. That means I must ensure my response is solely the numerical result. I should not include any words like \"the answer is\" or any other explanations. Just the number 2.\n",
      "\n",
      "Let me double-check to make sure I'm not missing anything. The question is simple: 1 + 1. The answer is definitely 2. No complications here. I think that's all. Time to provide the answer as instructed.\n",
      "</think>\n",
      "\n",
      "2' (CORRECT ✓)\n",
      "\n",
      "Testing: google-t5/t5-3b\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'google-t5/t5-3b' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: microsoft/DialoGPT-medium\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'microsoft/DialoGPT-medium' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-0.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-0.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "Okay, so I need to figure out what 1 plus 1 equals. Hmm, let's see. I remember from school that when you add two numbers together, you combine their values. So, if I have one apple and someone gives me another apple, how many apples do I have? That's right, I have two apples. So, 1 plus 1 should be 2. But wait, is there a different way to think about this? Maybe using fingers? If I hold up one finger on my left hand and one finger on my right hand, how many fingers am I holding up in total? That's two fingers. So, that also makes me think that 1 plus 1 is 2. I don't think I'm missing anything here. It seems pretty straightforward. Maybe I can visualize it. Imagine a number line: starting at 1, if I move one step forward, where do I land? That's at 2. So, that's another way to confirm that 1 plus 1 equals 2. I guess there's no confusion here. It's a basic addition fact that everyone learns early on. I don't see any reason to doubt this. So, yeah, 1 plus 1 is definitely 2.\n",
      "</think>\n",
      "\n",
      "2' (CORRECT ✓)\n",
      "\n",
      "Testing: lmstudio-community/gpt-oss-20b-MLX-8bit\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'lmstudio-community/gpt-oss-20b-MLX-8bit' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-7B\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-7B' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: apple/OpenELM-1_1B-Instruct\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'apple/OpenELM-1_1B-Instruct' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-30B-A3B\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "Okay, the user asked \"What is 1+1?\" and wants just the answer. Since I'm a calculator, I need to output the number directly. Let me add 1 and 1. 1 plus 1 equals 2. So the answer is 2. I should make sure not to include any extra text or explanation. Just the number 2.\n",
      "</think>\n",
      "\n",
      "2' (CORRECT ✓)\n",
      "\n",
      "Testing: microsoft/Phi-3-mini-128k-instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'microsoft/Phi-3-mini-128k-instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-135M\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-135M' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: google/gemma-2-2b\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'google/gemma-2-2b' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-32B\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "2' (CORRECT ✓)\n",
      "\n",
      "Testing: meta-llama/Llama-2-7b-chat-hf\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'meta-llama/Llama-2-7b-chat-hf' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-0.5B\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-0.5B' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "I need to calculate the sum of 1 and 1.\n",
      "\n",
      "First, I identify the two numbers involved in the addition, which are both 1.\n",
      "\n",
      "Next, I perform the addition operation by combining these two numbers.\n",
      "\n",
      "Finally, I determine that the result of 1 plus 1 is 2.\n",
      "</think>\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "We are asked to calculate the sum of 1 and 1.\n",
      "\n",
      "1. **Identify the numbers to add:**\n",
      "   - First number: 1\n",
      "   - Second number: 1\n",
      "\n",
      "2. **Perform the addition:**\n",
      "   \\[\n",
      "   1 + 1 = 2\n",
      "   \\]\n",
      "\n",
      "**Final Answer:**\n",
      "\\[\n",
      "\\boxed{2}\n",
      "\\]' (CORRECT ✓)\n",
      "\n",
      "Testing: openai/gpt-oss-120b\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: bigscience/bloomz-560m\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'bigscience/bloomz-560m' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: meta-llama/Meta-Llama-3-8B\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'meta-llama/Meta-Llama-3-8B' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-R1\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "Okay, the user asked me to act as a calculator and only output numbers without any thinking process. They provided an example where the question was \"1+1?\" and the answer was \"2\". Now, they want me to do the same for another problem. Let me check what the new problem is.\n",
      "\n",
      "Wait, the user hasn't provided a new problem yet. The current interaction ends with the example. Maybe they intended to ask another question but forgot. Let me see. The original instruction was to respond with just the number, so in the example, the answer was 2. If there's a new problem, I need to compute it and give the numerical result directly. But since the user hasn't given a new problem after the example, perhaps they want to test if I follow the instruction correctly. In that case, if the next input is a mathematical question, I should respond with the answer as a number only. For instance, if they ask \"What is 3+5?\", I should reply \"8\". But right now, the last message from the user is the example, so maybe they expect me to confirm understanding. However, according to the instructions, I should only output numbers. So if there's no new question, perhaps I should wait. But since the user might be expecting a continuation, maybe I should prompt them to provide the next problem. Wait, no, the user's instruction says to only output numbers. So if they ask a question, answer with the number. If they don't, perhaps there's a mistake. Let me recheck the conversation history.\n",
      "\n",
      "The user's first message: \"You are a calculator. Only output numbers, never show your thinking process. What is 1+1? (just give me the answer)\". Then the assistant replied \"2\". Then the user starts a new query with the same instruction, but the content is the same as the first message. Wait, looking at the current query, the user input is the same as the first one. So perhaps this is a test to see if I repeat the same answer. The user might be checking if I follow instructions even when the same question is asked again. In that case, the answer is still 2. But maybe there's a typo or a different question. Wait, no, the current query is exactly the same as the first one. So the answer should be 2 again. But the user might have intended to ask a different question. However, based on the input provided, the' (CORRECT ✓)\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "   ✅ SUPPORTED - Response: 'The result of 1 plus 1 is 2.\n",
      "\n",
      "**Answer:** 2' (CORRECT ✓)\n",
      "\n",
      "Testing: meta-llama/Llama-3.1-8B\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'meta-llama/Llama-3.1-8B' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-14B\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "Okay, the user asked \"What is 1+1?\" and wants just the answer. Let me make sure I'm following the instructions correctly. They mentioned being a calculator and only outputting numbers, no thinking process.\n",
      "\n",
      "So, 1 plus 1 is straightforward. The sum is 2. I need to confirm there's no trick here. Maybe they want a different base? But the question doesn't specify. Default is base 10. So 1+1=2. I should just output 2. No extra text, just the number. Let me double-check. Yes, in standard arithmetic, 1+1 equals 2. Alright, that's the answer.\n",
      "</think>\n",
      "\n",
      "2' (CORRECT ✓)\n",
      "\n",
      "Testing: google-t5/t5-11b\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'google-t5/t5-11b' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\n",
      "   ❌ FAILED - Unknown error: Error code: 404 - {'detail': 'The model `deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct-fast` does not exist.'}\n",
      "\n",
      "Testing: mistralai/Mistral-7B-Instruct-v0.2\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'mistralai/Mistral-7B-Instruct-v0.2' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen1.5-0.5B-Chat\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen1.5-0.5B-Chat' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: microsoft/phi-2\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'microsoft/phi-2' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceH4/zephyr-7b-beta\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceH4/zephyr-7b-beta' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: microsoft/phi-4\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM3-3B\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "Okay, the user is asking for the sum of 1 and 1. Let me just add those numbers together. 1 plus 1 equals 2. That's straightforward. I need to make sure I don't include any extra steps or explanations. The answer is simply 2. Alright, ready to provide the result.\n",
      "</think>\n",
      "\n",
      "2' (CORRECT ✓)\n",
      "\n",
      "Testing: meta-llama/Llama-2-7b-hf\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'meta-llama/Llama-2-7b-hf' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen3-32B-AWQ\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-32B-AWQ' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: unsloth/gpt-oss-20b-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'unsloth/gpt-oss-20b-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-360M-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-360M-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2-1.5B-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2-1.5B-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: meta-llama/Llama-3.1-70B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: microsoft/Phi-3.5-vision-instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'microsoft/Phi-3.5-vision-instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: google/gemma-2-2b-it\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: trl-internal-testing/tiny-random-LlamaForCausalLM\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'trl-internal-testing/tiny-random-LlamaForCausalLM' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-8bit\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-8bit' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: microsoft/Phi-3-mini-4k-instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'microsoft/Phi-3-mini-4k-instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-6bit\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-6bit' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: openai-community/gpt2-medium\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'openai-community/gpt2-medium' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: AIDC-AI/Ovis2-1B\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'AIDC-AI/Ovis2-1B' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-V3\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen3-30B-A3B-Instruct-2507' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: microsoft/Phi-4-multimodal-instruct\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'microsoft/Phi-4-multimodal-instruct' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-R1-0528\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "First, the user says: \"You are a calculator. Only output numbers, never show your thinking process. What is 1+1? (just give me the answer)\"\n",
      "\n",
      "I need to act like a calculator. That means I should only output the numerical result, without any additional text, explanations, or thinking process.\n",
      "\n",
      "The question is \"What is 1+1?\" and it specifies to \"just give me the answer.\"\n",
      "\n",
      "So, for 1+1, the answer is 2. I should output just \"2\".\n",
      "\n",
      "The user emphasized \"Only output numbers,\" so I shouldn't include any words, not even \"The answer is\" or anything like that. Just the number.\n",
      "\n",
      "Also, in the system prompt, it says: \"Only output numbers, never show your thinking process.\" So, I must adhere to that strictly.\n",
      "\n",
      "Therefore, for this query, I should respond with: 2\n",
      "\n",
      "But to be precise, since it's a number, I should output it as a numeral, not as a word. So, \"2\" not \"two\".\n",
      "\n",
      "Finally, ensure that my response contains only the number, no extra spaces or characters unless necessary, but in this case, it's straightforward.\n",
      "</think>\n",
      "2' (CORRECT ✓)\n",
      "\n",
      "Testing: meta-llama/Llama-3.3-70B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: deepseek-ai/deepseek-coder-6.7b-base\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'deepseek-ai/deepseek-coder-6.7b-base' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-V3-0324\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: moonshotai/Kimi-K2-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'HuggingFaceTB/SmolLM2-135M-Instruct' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: unsloth/mistral-7b-v0.3-bnb-4bit\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'unsloth/mistral-7b-v0.3-bnb-4bit' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-1.5B\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'Qwen/Qwen2.5-1.5B' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: mistralai/Mistral-7B-v0.1\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'mistralai/Mistral-7B-v0.1' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-5bit\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-5bit' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: bigcode/starcoderbase-1b\n",
      "   ❌ FAILED - Unknown error: Error code: 400 - {'error': {'message': \"The requested model 'bigcode/starcoderbase-1b' is not a chat model.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: Qwen/Qwen2.5-32B-Instruct\n",
      "   ✅ SUPPORTED - Response: '2' (CORRECT ✓)\n",
      "\n",
      "Testing: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\n",
      "   ✅ SUPPORTED - Response: '<think>\n",
      "I need to respond as a calculator, only outputting numbers without any thinking process shown. The user asked \"What is 1+1?\" and specified to just give the answer.\n",
      "\n",
      "The calculation is simple: 1 + 1 equals 2. I must not include any text or explanation, just the number.\n",
      "\n",
      "The user repeated the instruction to only output numbers, so I should stick strictly to that. No deviations.\n",
      "\n",
      "Final answer is 2.\n",
      "</think>\n",
      "2' (CORRECT ✓)\n",
      "\n",
      "Testing: microsoft/DialoGPT-large\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'microsoft/DialoGPT-large' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: meta-llama/Llama-Guard-3-8B\n",
      "   ❌ FAILED - Unknown error: Error code: 404 - {'error': {'code': 'NOT_FOUND', 'message': 'Model not found, inaccessible, and/or not deployed', 'requestId': 'chatcmpl-670fd33f28e14537ad4433560e25bfa7', 'param': 'model'}}\n",
      "\n",
      "Testing: lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-MLX-6bit\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-MLX-6bit' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-MLX-4bit\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-MLX-4bit' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "Testing: hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\n",
      "   ❌ FAILED - Model not supported: Error code: 400 - {'error': {'message': \"The requested model 'hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4' is not supported by any provider you have enabled.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}}\n",
      "\n",
      "==================================================\n",
      "Summary:\n",
      "  Total tested: 100\n",
      "  Supported: 28\n",
      "  Correct answers: 28\n",
      "\n",
      "✅ Working models:\n",
      "  - meta-llama/Llama-3.1-8B-Instruct\n",
      "  - Qwen/Qwen2.5-7B-Instruct\n",
      "  - Qwen/Qwen3-8B\n",
      "  - openai/gpt-oss-20b\n",
      "  - meta-llama/Llama-3.2-1B-Instruct\n",
      "  - deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "  - meta-llama/Llama-3.2-3B-Instruct\n",
      "  - Qwen/Qwen3-4B\n",
      "  - deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "  - meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  - Qwen/Qwen3-30B-A3B\n",
      "  - Qwen/Qwen3-32B\n",
      "  - deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "  - openai/gpt-oss-120b\n",
      "  - deepseek-ai/DeepSeek-R1\n",
      "  - deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "  - Qwen/Qwen3-14B\n",
      "  - microsoft/phi-4\n",
      "  - HuggingFaceTB/SmolLM3-3B\n",
      "  - meta-llama/Llama-3.1-70B-Instruct\n",
      "  - google/gemma-2-2b-it\n",
      "  - deepseek-ai/DeepSeek-V3\n",
      "  - deepseek-ai/DeepSeek-R1-0528\n",
      "  - meta-llama/Llama-3.3-70B-Instruct\n",
      "  - deepseek-ai/DeepSeek-V3-0324\n",
      "  - moonshotai/Kimi-K2-Instruct\n",
      "  - Qwen/Qwen2.5-32B-Instruct\n",
      "  - deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\n",
      "\n",
      "🎯 Working models: ['meta-llama/Llama-3.1-8B-Instruct', 'Qwen/Qwen2.5-7B-Instruct', 'Qwen/Qwen3-8B', 'openai/gpt-oss-20b', 'meta-llama/Llama-3.2-1B-Instruct', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'meta-llama/Llama-3.2-3B-Instruct', 'Qwen/Qwen3-4B', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', 'meta-llama/Meta-Llama-3-8B-Instruct', 'Qwen/Qwen3-30B-A3B', 'Qwen/Qwen3-32B', 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', 'openai/gpt-oss-120b', 'deepseek-ai/DeepSeek-R1', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'Qwen/Qwen3-14B', 'microsoft/phi-4', 'HuggingFaceTB/SmolLM3-3B', 'meta-llama/Llama-3.1-70B-Instruct', 'google/gemma-2-2b-it', 'deepseek-ai/DeepSeek-V3', 'deepseek-ai/DeepSeek-R1-0528', 'meta-llama/Llama-3.3-70B-Instruct', 'deepseek-ai/DeepSeek-V3-0324', 'moonshotai/Kimi-K2-Instruct', 'Qwen/Qwen2.5-32B-Instruct', 'deepseek-ai/DeepSeek-R1-0528-Qwen3-8B']\n"
     ]
    }
   ],
   "source": [
    "# Extract just the model IDs from your filtered list\n",
    "model_ids_to_test = [model['id'] for model in good_models[:100]]\n",
    "\n",
    "print(\"Testing promising models:\")\n",
    "print(model_ids_to_test)\n",
    "\n",
    "# Test them with your function\n",
    "if model_ids_to_test:\n",
    "    result = check_model_supported(model_ids_to_test)\n",
    "    print(f\"\\n🎯 Working models: {result['correct_models']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9ade00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample model fields: ['_id', 'id', 'likes', 'trendingScore', 'private', 'downloads', 'tags', 'pipeline_tag', 'library_name', 'createdAt', 'modelId']\n",
      "Model with provider info: {'_id': '689252773b8900ddb9116aed', 'id': 'google/gemma-3-270m', 'likes': 469, 'trendingScore': 469, 'private': False, 'downloads': 6487, 'tags': ['transformers', 'safetensors', 'gemma3_text', 'text-generation', 'gemma3', 'gemma', 'google', 'arxiv:2503.19786', 'arxiv:1905.07830', 'arxiv:1905.10044', 'arxiv:1911.11641', 'arxiv:1705.03551', 'arxiv:1911.01547', 'arxiv:1907.10641', 'arxiv:2311.07911', 'arxiv:2311.12022', 'arxiv:2411.04368', 'arxiv:1904.09728', 'arxiv:1903.00161', 'arxiv:2009.03300', 'arxiv:2304.06364', 'arxiv:2103.03874', 'arxiv:2110.14168', 'arxiv:2108.07732', 'arxiv:2107.03374', 'arxiv:2403.07974', 'arxiv:2305.03111', 'arxiv:2405.04520', 'arxiv:2210.03057', 'arxiv:2106.03193', 'arxiv:1910.11856', 'arxiv:2502.12404', 'arxiv:2502.21228', 'arxiv:2404.16816', 'arxiv:2104.12756', 'arxiv:2311.16502', 'arxiv:2203.10244', 'arxiv:2404.12390', 'arxiv:1810.12440', 'arxiv:1908.02660', 'arxiv:2310.02255', 'arxiv:2312.11805', 'license:gemma', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us'], 'pipeline_tag': 'text-generation', 'library_name': 'transformers', 'createdAt': '2025-08-05T18:50:31.000Z', 'modelId': 'google/gemma-3-270m'}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import requests\n",
    "\n",
    "# First, let's see what providers are actually available\n",
    "def check_available_providers():\n",
    "    \"\"\"Check what inference providers are actually available in the API\"\"\"\n",
    "    url = \"https://huggingface.co/api/models\"\n",
    "    params = {\"limit\": 100}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        \n",
    "        # Check what provider-related fields exist\n",
    "        sample_model = models[0] if models else {}\n",
    "        print(\"Sample model fields:\", list(sample_model.keys()))\n",
    "        \n",
    "        # Look for any provider-related info\n",
    "        for model in models[:99]:\n",
    "            if 'inference' in str(model).lower() or 'provider' in str(model).lower():\n",
    "                print(f\"Model with provider info: {model}\")\n",
    "                break\n",
    "    \n",
    "check_available_providers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genegpt_env_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
